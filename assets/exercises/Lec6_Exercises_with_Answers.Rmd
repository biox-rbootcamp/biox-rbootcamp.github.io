---
title: "Lecture 6: Exercises with answers"
date: October 16th, 2018
output: 
  html_notebook:
    toc: true
    toc_float: true
---


```{r, message=FALSE, warning = FALSE}
library(tidyverse)
```



# Exercise 1: Linear Regression

## Part 1

The data in the following URL "http://www-bcf.usc.edu/~gareth/ISL/Income1.csv"
includes observation on income levels (in tens of thousands of dollars)
and years of education. *The data is not real and was actually simulated*.

a. Read the data into R and generate a ggplot with a fitted line.
```{r}
income <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Income1.csv")
income <- income %>% select(-X1)
income
```

```{r}
ggplot(income, aes(x = Education, y = Income)) +
  geom_point() + geom_smooth(method = "lm")
```

b. Split the data into train and test set. Note that here we do not form
a validation set as we have very few observations and will only consider
a single model.

```{r}
set.seed(12345)
idx <- sample(1:nrow(income), floor(0.7 * nrow(income)))
income_train <-  income[idx, ]
income_test <- income[-idx, ]
```


c. Fit a linear model with education (years of education) as input variable 
and income as response variable. What are the model coefficients obtained and 
how can you extract them? Inspect the model fit using `summary()`

![[Source: Introduction to Statistical Learning](https://link-springer-com.stanford.idm.oclc.org/chapter/10.1007/978-1-4614-7138-7_2)](./education_income.png)


```{r}
fit <- lm(data = income_train, formula = Income ~ Education)
summary(fit)
```


```{r}
coef(fit)
```

d. Compute the fitted values of income for the observations
included in the train set.

```{r}
library(modelr)
income_train <- income_train %>%
    add_predictions(fit)
income_train
```


e. Predict the income for new observations, for people with 
16.00, 12.52, 15.55, 21.09, and 18.36 years of education. Then,
make predictions also for the test set and evaluate the root mean squared 
error on the test set.
```{r}
newobs <- data.frame(Education = c(16.00, 12.52, 15.55, 21.09, 18.36))
newobs <- newobs %>%
    add_predictions(fit)
newobs
```

```{r}
income_test <- income_test %>% add_predictions(fit)
(RMSE <- sqrt(mean((income_test$Income - income_test$pred)^2)))
```



## Part 2

a. Now download data from "http://www-bcf.usc.edu/~gareth/ISL/Income2.csv"
which include the same observations but also records data on "senority".
Again, split the data into train and test.

```{r}
income2 <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Income2.csv")
(income2 <- income2 %>% select(-X1))
```

```{r}
set.seed(12345)
idx <- sample(1:nrow(income2), floor(0.7 * nrow(income)))
income2_train <-  income2[idx, ]
income2_test <- income2[-idx, ]
```


b. Fit a new model including a new variable and print the model summary.

![[Source: Introduction to Statistical Learning](https://link-springer-com.stanford.idm.oclc.org/chapter/10.1007/978-1-4614-7138-7_2)](./education_seniority_income.png)

```{r}
mfit <- lm(data = income2_train, formula = Income ~ Education + Seniority)
summary(mfit)
```

c. Predicted values of income for the observations in the train set.

```{r}
(income2_train <- income2_train %>% add_predictions(mfit))
```


d. Evaluate the RMSE of the model on the test set.

```{r}
income2_test <- income2_test %>% add_predictions(mfit)
(RMSE_mfit <- sqrt(mean((income2_test$Income - income2_test$pred)^2)))
```

Note that since we don't have the validation test, we cannot use these RMSE
to choose between the two models. If you were to do that, you would 
need to first start with the same dataset, and split them into three
parts, use RMSE on a validation set to pick between the two models,
and evaluate the performance of the chosen model on the test set separately.

## Exercise 2

In this exercise you will perform Lasso regression yourself.
We will use the `Boston` dataset from the `MASS` package.
The dataset contains information on the Boston suburbs 
housing market collected by David Harrison in 1978.

We will try to predict the median value of of homes in the region based on 
its attributes recorded in other variables.

First install the package:
```{r}
# install.packages("MASS")
```

```{r}
# We are not loading the entire library MASS because we are using only 
# one dataset (also MASS has a select function which is in conflict with 
# dplyr::select)
Boston <- MASS::Boston
head(Boston, 3)
str(Boston)
```


a. Split the data to training and testing subsets.

```{r}
set.seed(123)
idx <- sample(1:nrow(Boston), round(0.7 * nrow(Boston)))
boston_train <- Boston[idx, ]
boston_test <- Boston[-idx, ]
```


b. Perform a Lasso regression with `glmnet`. Steps:
  
1. Extract the input and output data from the `Boston` `data.frame` and convert
them if necessary to a correct format.

```{r}
X_train <- data.matrix(boston_train[, -ncol(boston_train)])
y_train <- boston_train$medv
X_test <- data.matrix(boston_test[, -ncol(boston_test)])
y_test <- boston_test$medv
```


2. Use cross-validation to select the value for $\lambda$.

```{r}
library(glmnet)
cvfit <- cv.glmnet(X_train, y_train, nfolds = 5)
plot(cvfit)
```

3. Inspect computed coefficients for `lambda.min` and `lambda.1se`.

```{r}
cvfit$lambda.min
```

```{r}
cvfit$lambda.1se
```

4. Compute the predictions for the test dataset the two choices of the tuning
parameter, `lambda.min` and `lambda.1se`. 
Evaluate the MSE for each.

```{r}
final_pred <- predict(cvfit, newx=X_test, s="lambda.1se")
head(final_pred)
```

```{r}
(RMSE <- sqrt(mean((final_pred - y_test)^2)))
```

