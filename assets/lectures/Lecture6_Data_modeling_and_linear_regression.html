<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Lan Huong Nguyen" />
  <meta name="dcterms.date" content="2018-10-16" />
  <title>Lecture 6: Data modeling and linear regression</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="libs/reveal.js-3.3.0.1/css/reveal.css"/>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="libs/reveal.js-3.3.0.1/css/theme/simple.css" id="theme">


  <!-- some tweaks to reveal css -->
  <style type="text/css">
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }



    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }


  </style>

    <style type="text/css">code{white-space: pre;}</style>

    <link rel="stylesheet" href="cme195.css"/>

<!-- Printing and PDF exports -->
<script id="paper-css" type="application/dynamic-css">

/* Default Print Stylesheet Template
   by Rob Glazebrook of CSSnewbie.com
   Last Updated: June 4, 2008

   Feel free (nay, compelled) to edit, append, and
   manipulate this file as you see fit. */


@media print {

	/* SECTION 1: Set default width, margin, float, and
	   background. This prevents elements from extending
	   beyond the edge of the printed page, and prevents
	   unnecessary background images from printing */
	html {
		background: #fff;
		width: auto;
		height: auto;
		overflow: visible;
	}
	body {
		background: #fff;
		font-size: 20pt;
		width: auto;
		height: auto;
		border: 0;
		margin: 0 5%;
		padding: 0;
		overflow: visible;
		float: none !important;
	}

	/* SECTION 2: Remove any elements not needed in print.
	   This would include navigation, ads, sidebars, etc. */
	.nestedarrow,
	.controls,
	.fork-reveal,
	.share-reveal,
	.state-background,
	.reveal .progress,
	.reveal .backgrounds {
		display: none !important;
	}

	/* SECTION 3: Set body font face, size, and color.
	   Consider using a serif font for readability. */
	body, p, td, li, div {
		font-size: 20pt!important;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		color: #000;
	}

	/* SECTION 4: Set heading font face, sizes, and color.
	   Differentiate your headings from your body text.
	   Perhaps use a large sans-serif for distinction. */
	h1,h2,h3,h4,h5,h6 {
		color: #000!important;
		height: auto;
		line-height: normal;
		font-family: Georgia, "Times New Roman", Times, serif !important;
		text-shadow: 0 0 0 #000 !important;
		text-align: left;
		letter-spacing: normal;
	}
	/* Need to reduce the size of the fonts for printing */
	h1 { font-size: 28pt !important;  }
	h2 { font-size: 24pt !important; }
	h3 { font-size: 22pt !important; }
	h4 { font-size: 22pt !important; font-variant: small-caps; }
	h5 { font-size: 21pt !important; }
	h6 { font-size: 20pt !important; font-style: italic; }

	/* SECTION 5: Make hyperlinks more usable.
	   Ensure links are underlined, and consider appending
	   the URL to the end of the link for usability. */
	a:link,
	a:visited {
		color: #000 !important;
		font-weight: bold;
		text-decoration: underline;
	}
	/*
	.reveal a:link:after,
	.reveal a:visited:after {
		content: " (" attr(href) ") ";
		color: #222 !important;
		font-size: 90%;
	}
	*/


	/* SECTION 6: more reveal.js specific additions by @skypanther */
	ul, ol, div, p {
		visibility: visible;
		position: static;
		width: auto;
		height: auto;
		display: block;
		overflow: visible;
		margin: 0;
		text-align: left !important;
	}
	.reveal pre,
	.reveal table {
		margin-left: 0;
		margin-right: 0;
	}
	.reveal pre code {
		padding: 20px;
		border: 1px solid #ddd;
	}
	.reveal blockquote {
		margin: 20px 0;
	}
	.reveal .slides {
		position: static !important;
		width: auto !important;
		height: auto !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 0 !important;
		zoom: 1 !important;

		overflow: visible !important;
		display: block !important;

		text-align: left !important;
		-webkit-perspective: none;
		   -moz-perspective: none;
		    -ms-perspective: none;
		        perspective: none;

		-webkit-perspective-origin: 50% 50%;
		   -moz-perspective-origin: 50% 50%;
		    -ms-perspective-origin: 50% 50%;
		        perspective-origin: 50% 50%;
	}
	.reveal .slides section {
		visibility: visible !important;
		position: static !important;
		width: auto !important;
		height: auto !important;
		display: block !important;
		overflow: visible !important;

		left: 0 !important;
		top: 0 !important;
		margin-left: 0 !important;
		margin-top: 0 !important;
		padding: 60px 20px !important;
		z-index: auto !important;

		opacity: 1 !important;

		page-break-after: always !important;

		-webkit-transform-style: flat !important;
		   -moz-transform-style: flat !important;
		    -ms-transform-style: flat !important;
		        transform-style: flat !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;

		-webkit-transition: none !important;
		   -moz-transition: none !important;
		    -ms-transition: none !important;
		        transition: none !important;
	}
	.reveal .slides section.stack {
		padding: 0 !important;
	}
	.reveal section:last-of-type {
		page-break-after: avoid !important;
	}
	.reveal section .fragment {
		opacity: 1 !important;
		visibility: visible !important;

		-webkit-transform: none !important;
		   -moz-transform: none !important;
		    -ms-transform: none !important;
		        transform: none !important;
	}
	.reveal section img {
		display: block;
		margin: 15px 0px;
		background: rgba(255,255,255,1);
		border: 1px solid #666;
		box-shadow: none;
	}

	.reveal section small {
		font-size: 0.8em;
	}

}  
</script>


<script id="pdf-css" type="application/dynamic-css">
    
/**
 * This stylesheet is used to print reveal.js
 * presentations to PDF.
 *
 * https://github.com/hakimel/reveal.js#pdf-export
 */

* {
	-webkit-print-color-adjust: exact;
}

body {
	margin: 0 auto !important;
	border: 0;
	padding: 0;
	float: none !important;
	overflow: visible;
}

html {
	width: 100%;
	height: 100%;
	overflow: visible;
}

/* Remove any elements not needed in print. */
.nestedarrow,
.reveal .controls,
.reveal .progress,
.reveal .playback,
.reveal.overview,
.fork-reveal,
.share-reveal,
.state-background {
	display: none !important;
}

h1, h2, h3, h4, h5, h6 {
	text-shadow: 0 0 0 #000 !important;
}

.reveal pre code {
	overflow: hidden !important;
	font-family: Courier, 'Courier New', monospace !important;
}

ul, ol, div, p {
	visibility: visible;
	position: static;
	width: auto;
	height: auto;
	display: block;
	overflow: visible;
	margin: auto;
}
.reveal {
	width: auto !important;
	height: auto !important;
	overflow: hidden !important;
}
.reveal .slides {
	position: static;
	width: 100%;
	height: auto;

	left: auto;
	top: auto;
	margin: 0 !important;
	padding: 0 !important;

	overflow: visible;
	display: block;

	-webkit-perspective: none;
	   -moz-perspective: none;
	    -ms-perspective: none;
	        perspective: none;

	-webkit-perspective-origin: 50% 50%; /* there isn't a none/auto value but 50-50 is the default */
	   -moz-perspective-origin: 50% 50%;
	    -ms-perspective-origin: 50% 50%;
	        perspective-origin: 50% 50%;
}

.reveal .slides section {
	page-break-after: always !important;

	visibility: visible !important;
	position: relative !important;
	display: block !important;
	position: relative !important;

	margin: 0 !important;
	padding: 0 !important;
	box-sizing: border-box !important;
	min-height: 1px;

	opacity: 1 !important;

	-webkit-transform-style: flat !important;
	   -moz-transform-style: flat !important;
	    -ms-transform-style: flat !important;
	        transform-style: flat !important;

	-webkit-transform: none !important;
	   -moz-transform: none !important;
	    -ms-transform: none !important;
	        transform: none !important;
}

.reveal section.stack {
	margin: 0 !important;
	padding: 0 !important;
	page-break-after: avoid !important;
	height: auto !important;
	min-height: auto !important;
}

.reveal img {
	box-shadow: none;
}

.reveal .roll {
	overflow: visible;
	line-height: 1em;
}

/* Slide backgrounds are placed inside of their slide when exporting to PDF */
.reveal section .slide-background {
	display: block !important;
	position: absolute;
	top: 0;
	left: 0;
	width: 100%;
	z-index: -1;
}

/* All elements should be above the slide-background */
.reveal section>* {
	position: relative;
	z-index: 1;
}

/* Display slide speaker notes when 'showNotes' is enabled */
.reveal .speaker-notes-pdf {
	display: block;
	width: 100%;
	max-height: none;
	left: auto;
	top: auto;
	z-index: 100;
}

/* Display slide numbers when 'slideNumber' is enabled */
.reveal .slide-number-pdf {
	display: block;
	position: absolute;
	font-size: 14px;
}

</script>


<script>
var style = document.createElement( 'style' );
style.type = 'text/css';
var style_script_id = window.location.search.match( /print-pdf/gi ) ? 'pdf-css' : 'paper-css';
var style_script = document.getElementById(style_script_id).text;
style.innerHTML = style_script;
document.getElementsByTagName('head')[0].appendChild(style);
</script>

</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
    <h1 class="title">Lecture 6: Data modeling and linear regression</h1>
  <h1 class="subtitle">CME/STATS 195</h1>
    <h2 class="author">Lan Huong Nguyen</h2>
    <h3 class="date">October 16, 2018</h3>
</section>

<section id="contents" class="slide level2">
<h2>Contents</h2>
<div class="left" ,="" style="width: 50%">
<ul>
<li><p>Data Modeling</p></li>
<li><p>Linear Regression</p></li>
<li><p>Lasso Regression</p></li>
</ul>
</div>
<div class="right" ,="" style="width: 50%">
<p><img src="Lecture6-figure/data-science-model.png" /></p>
</div>
</section>
<section><section id="data-modeling" class="titleslide slide level1"><h1>Data Modeling</h1></section><section id="introduction-to-models" class="slide level2">
<h2>Introduction to models</h2>
<blockquote>
<p>“<strong>All models are wrong, but some are useful</strong>. Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, <strong>cunningly chosen parsimonious models often do provide remarkably useful approximations</strong> (…). For such a model there is no need to ask the question”Is the model true?“. If”truth&quot; is to be the “whole truth” the answer must be “No”. The only question of interest is “Is the model illuminating and useful?” – George E.P. Box, 1976</p>
</blockquote>
<ul>
<li><p>The goal of a model is to <strong>provide a simple low-dimensional summary of a dataset</strong>.</p></li>
<li><p>Models can be used to <strong>partition data into patterns of interest and residuals</strong> (other sources of variation and random noise).</p></li>
</ul>
</section><section id="hypothesis-generation-vs.hypothesis-confirmation" class="slide level2">
<h2>Hypothesis generation vs. hypothesis confirmation</h2>
<p></br></p>
<ul>
<li><p>Usually models are used for inference or confirmation of a pre-specified hypothesis.</p></li>
<li><p>Doing inference correctly is hard. The key idea you must understand is that: <strong>Each observation can either be used for exploration or confirmation, NOT both.</strong></p></li>
<li><p>Observation can be used many times for exploration, but only once for confirmation.</p></li>
<li><p>There is nothing wrong with exploration, but you should <strong>never sell an exploratory analysis as a confirmatory analysis</strong> because it is fundamentally misleading.</p></li>
</ul>
</section><section id="confirmatory-analysis" class="slide level2">
<h2>Confirmatory analysis</h2>
<p>If you plan to do confirmatory analysis at some point after EDA, one approach is to split your data into three pieces before you begin the analysis:</p>
<ul>
<li><p><strong>Training set</strong> – the bulk (e.g. 60%) of the dataset which can be used to do anything: visualizing, fitting multiple models.</p></li>
<li><p><strong>Validation set</strong> – a smaller set (e.g. 20%) used for manually comparing models and visualizations.</p></li>
<li><p><strong>Test set</strong> – a set (e.g. 20%) held back used only ONCE to test and asses your final model.</p></li>
</ul>
</section><section id="confirmatory-analysis-1" class="slide level2">
<h2>Confirmatory analysis</h2>
<ul>
<li><p>Partitioning the dataset allows you to explore the training data, generate a number of candidate hypotheses and models.</p></li>
<li><p>You can select a final model based on its performance on the validation set.</p></li>
<li><p>Finally, when you are confident with the chosen model you can check how good it is using the test data.</p></li>
<li><p><em>Note that even when doing confirmatory modeling, you will still need to do EDA. If you don’t do any EDA you might remain blind to some quality problems with your data.</em></p></li>
</ul>
</section><section id="model-basics" class="slide level2">
<h2>Model Basics</h2>
<p></br></p>
<p>There are two parts to data modeling:</p>
<ul>
<li><strong>defining a family of models</strong>: deciding on a set of models that can express a type of pattern you want to capture, e.g. a straight line, or a quadratic curve.</li>
<li><strong>fitting a model</strong>: finding a model within the family that the closest to your data.</li>
</ul>
<p></br></p>
<p>A fitted model is just the best model from a chosen family of models, i.e. the “best” according to some set criteria.</p>
<p>This does not necessarily imply that the model is a good and certainly does NOT imply that the model is true.</p>
</section><section id="the-modelr-package" class="slide level2">
<h2>The <code>modelr</code> package</h2>
<p></br></p>
<ul>
<li><p>The <code>modelr</code> package, provides a few useful functions that are wrappers around base R’s modeling functions.</p></li>
<li><p>These functions facilitate the data analysis process as they are nicely integrated with the <code>tidyverse</code> pipeline.</p></li>
<li><p><code>modelr</code> is not automatically loaded when you load in <code>tidyverse</code> package, you need to do it separately:</p></li>
</ul>
<p></br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(modelr)</code></pre></div>
</section><section id="a-toy-dataset" class="slide level2">
<h2>A toy dataset</h2>
<p>We will work with a simulated dataset <code>sim1</code> from <code>modelr</code>:</p>
<div class="left" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim1</code></pre></div>
<pre><code>## # A tibble: 30 x 2
##        x     y
##    &lt;int&gt; &lt;dbl&gt;
##  1     1  4.20
##  2     1  7.51
##  3     1  2.13
##  4     2  8.99
##  5     2 10.2 
##  6     2 11.3 
##  7     3  7.36
##  8     3 10.5 
##  9     3 10.5 
## 10     4 12.4 
## # ... with 20 more rows</code></pre>
</div>
<div class="right" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sim1, <span class="kw">aes</span>(x, y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="Lecture6_Data_modeling_and_linear_regression_files/figure-revealjs/unnamed-chunk-3-1.png" width="768" /></p>
</div>
</section><section id="defining-a-family-of-models" class="slide level2">
<h2>Defining a family of models</h2>
<p>The relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> for the points in <code>sim1</code> look linear. So, will look for models which belong to <strong>a family of models</strong> of the following form:</p>
<div class="left" ,="" style="width: 50%">
<p></br></p>
<p><span class="math display">\[y= \beta_0 + \beta_1 \cdot x\]</span></p>
<p></br></p>
<p>The models that can be expressed by the above formula, can adequately capture a linear trend.</p>
<p>We generate a few examples of the models from this family on the right.</p>
</div>
<div class="right" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models &lt;-<span class="st"> </span><span class="kw">tibble</span>(
    <span class="dt">b0 =</span> <span class="kw">runif</span>(<span class="dv">250</span>, <span class="op">-</span><span class="dv">20</span>, <span class="dv">40</span>),
    <span class="dt">b1 =</span> <span class="kw">runif</span>(<span class="dv">250</span>, <span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>))

<span class="kw">ggplot</span>(sim1, <span class="kw">aes</span>(x, y)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>(
      <span class="dt">data =</span> models, 
      <span class="kw">aes</span>(<span class="dt">intercept =</span> b0, <span class="dt">slope =</span> b1), 
      <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">4</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() </code></pre></div>
<p><img src="Lecture6_Data_modeling_and_linear_regression_files/figure-revealjs/unnamed-chunk-4-1.png" width="768" /></p>
</div>
</section><section id="fitting-a-model" class="slide level2">
<h2>Fitting a model</h2>
<p>From all the lines in the linear family of models, we need to find the best one, i.e. the one that is <strong>the closest to the data</strong>.</p>
<p>This means that we need to find parameters <span class="math inline">\(\hat a_0\)</span> and <span class="math inline">\(\hat a_1\)</span> that identify such a fitted line.</p>
<div class="left" ,="" style="width: 50%">
<p>The closest to the data can be defined as the one with the minimum distance to the data points in the <span class="math inline">\(y\)</span> direction (the minimum residuals):</p>
<span class="math display">\[\begin{align*}
\|\hat e\|^2_2 &amp;= \|\vec y - \hat y\|_2^2\\
&amp;= \|\vec y - (\hat \beta_0 + \hat \beta_1 x)\|_2^2\\
&amp;= \sum_{i = 1}^n (y_i - (\hat \beta_0 + \hat \beta_1 x_i))^2
\end{align*}\]</span>
</div>
<div class="right", style="width: 50%">

<p><img src="Lecture6_Data_modeling_and_linear_regression_files/figure-revealjs/unnamed-chunk-5-1.png" width="768" /></p>
</section></section>
<section><section id="linear-regression" class="titleslide slide level1"><h1>Linear Regression</h1></section><section id="linear-regression-1" class="slide level2">
<h2>Linear Regression</h2>
<p></br></p>
<ul>
<li class="fragment">Regression is a supervised learning method, whose goal is inferring the relationship between input data, <span class="math inline">\(x\)</span>, and a <strong>continuous</strong> response variable, <span class="math inline">\(y\)</span>.</li>
</ul>
<ul>
<li class="fragment">Linear regression is a type of regression where <strong><span class="math inline">\(y\)</span> is modeled as a linear function of <span class="math inline">\(x\)</span></strong>.</li>
</ul>
<ul>
<li class="fragment"><strong>Simple linear regression</strong> predicts the output <span class="math inline">\(y\)</span> from a single predictor <span class="math inline">\(x\)</span>. <span class="math display">\[y = \beta_0 + \beta_1 x + \epsilon\]</span></li>
</ul>
<ul>
<li class="fragment"><strong>Multiple linear regression</strong> assumes <span class="math inline">\(y\)</span> relies on many covariates:
<span class="math display">\[\begin{align*}
y &amp;= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon \\
&amp;= \boldsymbol{\beta}^T \boldsymbol{x} + \epsilon
\end{align*}\]</span></li>
</ul>
<ul>
<li class="fragment">here <span class="math inline">\(\epsilon\)</span> denotes a random noise term with zero mean.</li>
</ul>
</section><section id="objective-function" class="slide level2">
<h2>Objective function</h2>
<p>Linear regression seeks a solution <span class="math inline">\(\hat y = \hat \beta \cdot \vec x\)</span> that <strong>minimizes the difference between the true outcome <span class="math inline">\(y\)</span> and the prediction <span class="math inline">\(\hat y\)</span></strong>, in terms of the residual sum of squares (RSS).</p>
<p><span class="math display">\[
arg \min\limits_{\hat \beta} 
\sum_i \left(y_i - \boldsymbol{\hat \beta}^T \boldsymbol{x}_i\right)^2
\]</span></p>
</section><section id="simple-linear-regression" class="slide level2">
<h2>Simple Linear Regression</h2>
<ul>
<li><p>Predict the mileage per gallon using the weight of the car.</p></li>
<li><p>In R the linear models can be fit with a <code>lm()</code> function.</p></li>
</ul>
<div class="left" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># convert &#39;data.frame&#39; to &#39;tibble&#39;:</span>
mtcars &lt;-<span class="st"> </span><span class="kw">tbl_df</span>(mtcars)

<span class="co"># Separate the data into train and test:</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
n &lt;-<span class="st"> </span><span class="kw">nrow</span>(mtcars)
idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>n, <span class="dt">size =</span> <span class="kw">floor</span>(n<span class="op">/</span><span class="dv">2</span>))
mtcars_train &lt;-<span class="st"> </span>mtcars[idx, ]
mtcars_test &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>idx, ]

<span class="co"># Fit a simple linear model:</span>
mtcars_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>wt, mtcars_train)
<span class="co"># Extract the fitted model coefficients:</span>
<span class="kw">coef</span>(mtcars_fit)</code></pre></div>
<pre><code>## (Intercept)          wt 
##   36.469815   -5.406813</code></pre>
</div>
<div class="right" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check the details on the fitted model:</span>
<span class="kw">summary</span>(mtcars_fit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt, data = mtcars_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.5302 -1.9952  0.0179  1.3017  3.5194 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   36.470      2.108  17.299 7.61e-11 ***
## wt            -5.407      0.621  -8.707 5.04e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.2 on 14 degrees of freedom
## Multiple R-squared:  0.8441, Adjusted R-squared:  0.833 
## F-statistic: 75.81 on 1 and 14 DF,  p-value: 5.043e-07</code></pre>
</div>
</section><section id="fitted-values" class="slide level2">
<h2>Fitted values</h2>
<p>We can compute the fitted values <span class="math inline">\(\hat y\)</span>, a.k.a. the predicted <code>mpg</code> values for existing observations using <code>modelr::add_predictions()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_train &lt;-<span class="st"> </span>mtcars_train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">add_predictions</span>(mtcars_fit)
mtcars_train</code></pre></div>
<pre><code>## # A tibble: 16 x 12
##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb  pred
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  19.2     6 168.    123  3.92  3.44  18.3     1     0     4     4 17.9 
##  2  19.2     8 400     175  3.08  3.84  17.0     0     0     3     2 15.7 
##  3  17.3     8 276.    180  3.07  3.73  17.6     0     0     3     3 16.3 
##  4  27.3     4  79      66  4.08  1.94  18.9     1     1     4     1 26.0 
##  5  26       4 120.     91  4.43  2.14  16.7     0     1     5     2 24.9 
##  6  21       6 160     110  3.9   2.88  17.0     0     1     4     4 20.9 
##  7  15.2     8 276.    180  3.07  3.78  18       0     0     3     3 16.0 
##  8  15.2     8 304     150  3.15  3.44  17.3     0     0     3     2 17.9 
##  9  15.8     8 351     264  4.22  3.17  14.5     0     1     5     4 19.3 
## 10  17.8     6 168.    123  3.92  3.44  18.9     1     0     4     4 17.9 
## 11  15.5     8 318     150  2.76  3.52  16.9     0     0     3     2 17.4 
## 12  21.4     4 121     109  4.11  2.78  18.6     1     1     4     2 21.4 
## 13  13.3     8 350     245  3.73  3.84  15.4     0     0     3     4 15.7 
## 14  15       8 301     335  3.54  3.57  14.6     0     1     5     8 17.2 
## 15  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2 28.3 
## 16  10.4     8 460     215  3     5.42  17.8     0     0     3     4  7.14</code></pre>
</section><section id="predictions-for-new-observations" class="slide level2">
<h2>Predictions for new observations</h2>
<p>To predict the <code>mpg</code> for <strong>new observations</strong>, e.g. cars not in the dataset, we first need to generate a data table with predictors <span class="math inline">\(x\)</span>, in this case the car weights:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newcars &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">wt =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="fl">2.1</span>, <span class="fl">3.14</span>, <span class="fl">4.1</span>, <span class="fl">4.3</span>))
newcars &lt;-<span class="st"> </span>newcars <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">add_predictions</span>(mtcars_fit)
newcars</code></pre></div>
<pre><code>## # A tibble: 5 x 2
##      wt  pred
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  2     25.7
## 2  2.1   25.1
## 3  3.14  19.5
## 4  4.1   14.3
## 5  4.3   13.2</code></pre>
</section><section id="predictions-for-the-test-set" class="slide level2">
<h2>Predictions for the test set</h2>
<p>Remember that we already set aside a test set check our model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_test &lt;-<span class="st"> </span>mtcars_test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">add_predictions</span>(mtcars_fit)
<span class="kw">head</span>(mtcars_test, <span class="dv">3</span>)</code></pre></div>
<pre><code>## # A tibble: 3 x 12
##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb  pred
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4  22.3
## 2  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1  23.9
## 3  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1  19.1</code></pre>
<p>Compute the root mean square error:</p>
<p><span class="math display">\[
RMSE = \frac{1}{\sqrt{n}}\|\vec{y} - \vec{\hat y}\| = 
\sqrt{\frac{1}{n}\sum_{i = 1}^n(y_i - \hat y_i)^2}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">mean</span>((mtcars_test<span class="op">$</span>mpg <span class="op">-</span><span class="st"> </span>mtcars_test<span class="op">$</span>pred)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 3.902534</code></pre>
</section><section id="visualizing-the-model" class="slide level2">
<h2>Visualizing the model</h2>
<p>Now we can compare our predictions (grey) to the observed (black) values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(mtcars_train, <span class="kw">aes</span>(wt)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> mpg)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred), <span class="dt">fill =</span> <span class="st">&quot;grey&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">shape =</span> <span class="dv">21</span>, <span class="dt">size =</span> <span class="dv">2</span>) </code></pre></div>
<p><img src="Lecture6_Data_modeling_and_linear_regression_files/figure-revealjs/unnamed-chunk-12-1.png" width="768" /></p>
</section><section id="visualizing-the-residuals" class="slide level2">
<h2>Visualizing the residuals</h2>
<p><strong>The residuals tell you what the model has missed</strong>. We can compute and add residuals to data with <code>add_residuals()</code> from <code>modelr</code> package:</p>
<p>Plotting residuals is a good practice – you want the residuals to look like random noise.</p>
<div class="left" ,="" style="width: 40%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_train &lt;-<span class="st"> </span>mtcars_train <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">add_residuals</span>(mtcars_fit)
mtcars_train <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(mpg, mpg, resid, pred)</code></pre></div>
<pre><code>## # A tibble: 16 x 3
##      mpg   resid  pred
##    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
##  1  19.2  1.33   17.9 
##  2  19.2  3.52   15.7 
##  3  17.3  0.998  16.3 
##  4  27.3  1.29   26.0 
##  5  26    1.10   24.9 
##  6  21    0.0748 20.9 
##  7  15.2 -0.832  16.0 
##  8  15.2 -2.70   17.9 
##  9  15.8 -3.53   19.3 
## 10  17.8 -0.0704 17.9 
## 11  15.5 -1.94   17.4 
## 12  21.4 -0.0389 21.4 
## 13  13.3 -2.41   15.7 
## 14  15   -2.17   17.2 
## 15  30.4  2.11   28.3 
## 16  10.4  3.26    7.14</code></pre>
</div>
<div class="right" ,="" style="width: 60%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(mtcars_train, <span class="kw">aes</span>(wt, resid)) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_ref_line</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">colour =</span> <span class="st">&quot;grey&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() </code></pre></div>
<p><img src="Lecture6_Data_modeling_and_linear_regression_files/figure-revealjs/unnamed-chunk-14-1.png" width="768" /></p>
</div>
</section><section id="formulae-in-r" class="slide level2">
<h2>Formulae in R</h2>
<p>You have seen that <code>lm()</code> takes in a formula relation <code>y ~ x</code> as an argument.</p>
<p>You can take a look at what R actually does, you can use the <code>model_matrix()</code>.</p>
<div class="left" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim1</code></pre></div>
<pre><code>## # A tibble: 30 x 3
##        x     y  pred
##    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1     1  4.20  6.27
##  2     1  7.51  6.27
##  3     1  2.13  6.27
##  4     2  8.99  8.32
##  5     2 10.2   8.32
##  6     2 11.3   8.32
##  7     3  7.36 10.4 
##  8     3 10.5  10.4 
##  9     3 10.5  10.4 
## 10     4 12.4  12.4 
## # ... with 20 more rows</code></pre>
</div>
<div class="right" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">model_matrix</span>(sim1, y <span class="op">~</span><span class="st"> </span>x)</code></pre></div>
<pre><code>## # A tibble: 30 x 2
##    `(Intercept)`     x
##            &lt;dbl&gt; &lt;dbl&gt;
##  1             1     1
##  2             1     1
##  3             1     1
##  4             1     2
##  5             1     2
##  6             1     2
##  7             1     3
##  8             1     3
##  9             1     3
## 10             1     4
## # ... with 20 more rows</code></pre>
</div>
</section><section id="formulae-with-categorical-variables" class="slide level2">
<h2>Formulae with categorical variables</h2>
<ul>
<li><p>It doesn’t make sense to parametrize the model with categorical variables, as we did before.</p></li>
<li><p><code>trans</code> variable is not a number, so R creates <strong>an indicator column</strong> that is 1 if “male”, and 0 if “female”.</p></li>
</ul>
<div class="left" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(df &lt;-<span class="st"> </span><span class="kw">tibble</span>(
    <span class="dt">sex =</span> <span class="kw">c</span>(<span class="st">&quot;male&quot;</span>, <span class="st">&quot;female&quot;</span>, <span class="st">&quot;female&quot;</span>, 
            <span class="st">&quot;female&quot;</span>, <span class="st">&quot;male&quot;</span>, <span class="st">&quot;male&quot;</span>),
     <span class="dt">response =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">8</span>)
))</code></pre></div>
<pre><code>## # A tibble: 6 x 2
##   sex    response
##   &lt;chr&gt;     &lt;dbl&gt;
## 1 male          2
## 2 female        5
## 3 female        1
## 4 female        3
## 5 male          6
## 6 male          8</code></pre>
</div>
<div class="right" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">model_matrix</span>(df, response <span class="op">~</span><span class="st"> </span>sex)</code></pre></div>
<pre><code>## # A tibble: 6 x 2
##   `(Intercept)` sexmale
##           &lt;dbl&gt;   &lt;dbl&gt;
## 1             1       1
## 2             1       0
## 3             1       0
## 4             1       0
## 5             1       1
## 6             1       1</code></pre>
</div>
</section><section id="section" class="slide level2">
<h2></h2>
<ul>
<li>In general, it creates k−1 columns, where k is the number of categories.</li>
</ul>
<div class="left" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(df &lt;-<span class="st"> </span><span class="kw">tibble</span>(
    <span class="dt">rating =</span> <span class="kw">c</span>(<span class="st">&quot;good&quot;</span>, <span class="st">&quot;bad&quot;</span>, <span class="st">&quot;average&quot;</span>, <span class="st">&quot;bad&quot;</span>,
               <span class="st">&quot;average&quot;</span>, <span class="st">&quot;good&quot;</span>, <span class="st">&quot;bad&quot;</span>, <span class="st">&quot;good&quot;</span>),
    <span class="dt">score =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">10</span>, <span class="dv">6</span>)
))</code></pre></div>
<pre><code>## # A tibble: 8 x 2
##   rating  score
##   &lt;chr&gt;   &lt;dbl&gt;
## 1 good        2
## 2 bad         5
## 3 average     1
## 4 bad         3
## 5 average     6
## 6 good        8
## 7 bad        10
## 8 good        6</code></pre>
</div>
<div class="right" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">model_matrix</span>(df, score <span class="op">~</span><span class="st"> </span>rating)</code></pre></div>
<pre><code>## # A tibble: 8 x 3
##   `(Intercept)` ratingbad ratinggood
##           &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1             1         0          1
## 2             1         1          0
## 3             1         0          0
## 4             1         1          0
## 5             1         0          0
## 6             1         0          1
## 7             1         1          0
## 8             1         0          1</code></pre>
</div>
<div class="left" ,="" style="width: 100%">
<p>But you don’t need to worry about the parametrization to make predictions.</p>
</div>
</section><section id="multiple-linear-regression" class="slide level2">
<h2>Multiple Linear Regression</h2>
<p>Models often include <strong>multiple predictors</strong>, e.g. we might like to predict <code>mpg</code> using three variables: <code>wt</code>, <code>disp</code> and <code>cyl</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(mtcars, <span class="kw">aes</span>(<span class="dt">x=</span>wt, <span class="dt">y=</span>mpg, <span class="dt">col=</span>cyl, <span class="dt">size=</span>disp)) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">scale_color_viridis_c</span>()</code></pre></div>
<p><img src="Lecture6_Data_modeling_and_linear_regression_files/figure-revealjs/unnamed-chunk-21-1.png" width="1152" /></p>
</section><section id="section-1" class="slide level2">
<h2></h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_mult_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>disp <span class="op">+</span><span class="st"> </span>cyl, <span class="dt">data =</span> mtcars_train)

<span class="co"># Summarize the results </span>
<span class="kw">summary</span>(mtcars_mult_fit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + disp + cyl, data = mtcars_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.4016 -0.9539  0.0017  0.6243  3.4510 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 40.259994   2.692593  14.952 4.03e-09 ***
## wt          -3.986230   0.984659  -4.048  0.00162 ** 
## disp         0.009933   0.010756   0.924  0.37394    
## cyl         -1.644638   0.629635  -2.612  0.02272 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.818 on 12 degrees of freedom
## Multiple R-squared:  0.9088, Adjusted R-squared:  0.886 
## F-statistic: 39.88 on 3 and 12 DF,  p-value: 1.616e-06</code></pre>
</section><section id="section-2" class="slide level2">
<h2></h2>
<p>To <strong>predict <code>mpg</code> for new cars</strong>, you must first create a data frame describing the attributes of the new cars, before computing predicted <code>mpg</code> values.</p>
<div class="left" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newcars &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(
    <span class="dt">wt =</span> <span class="kw">c</span>(<span class="fl">2.1</span>, <span class="fl">3.6</span>, <span class="fl">5.1</span>), 
    <span class="dt">disp =</span> <span class="kw">c</span>(<span class="dv">150</span>, <span class="dv">250</span>), 
    <span class="dt">cyl =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">6</span>)
)
newcars</code></pre></div>
<pre><code>##     wt disp cyl
## 1  2.1  150   4
## 2  3.6  150   4
## 3  5.1  150   4
## 4  2.1  250   4
## 5  3.6  250   4
## 6  5.1  250   4
## 7  2.1  150   6
## 8  3.6  150   6
## 9  5.1  150   6
## 10 2.1  250   6
## 11 3.6  250   6
## 12 5.1  250   6</code></pre>
</div>
<div class="right" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newcars &lt;-<span class="st"> </span>newcars <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">add_predictions</span>(mtcars_mult_fit)
newcars</code></pre></div>
<pre><code>##     wt disp cyl     pred
## 1  2.1  150   4 26.80031
## 2  3.6  150   4 20.82097
## 3  5.1  150   4 14.84162
## 4  2.1  250   4 27.79361
## 5  3.6  250   4 21.81427
## 6  5.1  250   4 15.83492
## 7  2.1  150   6 23.51104
## 8  3.6  150   6 17.53169
## 9  5.1  150   6 11.55235
## 10 2.1  250   6 24.50434
## 11 3.6  250   6 18.52499
## 12 5.1  250   6 12.54565</code></pre>
</div>
</section><section id="predictions-for-the-test-set-1" class="slide level2">
<h2>Predictions for the test set</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_test_mult &lt;-<span class="st"> </span>mtcars_test <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">add_predictions</span>(mtcars_mult_fit)
<span class="kw">head</span>(mtcars_test_mult, <span class="dv">3</span>)</code></pre></div>
<pre><code>## # A tibble: 3 x 12
##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb  pred
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4  21.5
## 2  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1  25.5
## 3  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1  20.1</code></pre>
<p>Compute the root mean square error:</p>
<p><span class="math display">\[
RMSE = \frac{1}{\sqrt{n}}\|\vec{y} - \vec{\hat y}\| = 
\sqrt{\frac{1}{n}\sum_{i = 1}^n(y_i - \hat y_i)^2}
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">mean</span>((mtcars_test_mult<span class="op">$</span>mpg <span class="op">-</span><span class="st"> </span>mtcars_test_mult<span class="op">$</span>pred)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 3.172103</code></pre>
</section><section id="interaction-terms" class="slide level2">
<h2>Interaction terms</h2>
<ul>
<li><p>An interaction occurs when <strong>an independent variable has a different effect on the outcome depending on the values of another independent</strong>. variable.</p></li>
<li><p>For example, one variable, <span class="math inline">\(x_1\)</span> might have a different effect on <span class="math inline">\(y\)</span> within different categories or groups, given by variable <span class="math inline">\(x_2\)</span>.</p></li>
<li><p>If you are not familiar with the concept of the interaction terms, read <a href="http://www.medicine.mcgill.ca/epidemiology/joseph/courses/EPIB-621/interaction.pdf">this</a>.</p></li>
</ul>
</section><section id="formulas-with-interactions" class="slide level2">
<h2>Formulas with interactions</h2>
<p>In the <code>sim3</code> dataset, there is a categorical, <code>x2</code>, and a continuous, <code>x1</code>, predictor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sim3, <span class="kw">aes</span>(<span class="dt">x=</span>x1, <span class="dt">y=</span>y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> x2)) </code></pre></div>
<p><img src="Lecture6_Data_modeling_and_linear_regression_files/figure-revealjs/unnamed-chunk-27-1.png" width="768" /></p>
</section><section id="models-with-interactions" class="slide level2">
<h2>Models with interactions</h2>
<p>We could fit two different models, one without and one with (<code>mod2</code>) different slopes and intercepts for each line (for each <code>x2</code> category).</p>
<div class="left" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Model without interactions:</span>
mod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> sim3)    
<span class="co"># Model with interactions:</span>
mod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>x2, <span class="dt">data =</span> sim3)     
<span class="co"># Generate a data grid for two variables </span>
<span class="co"># and compute predictions from both models</span>
grid &lt;-<span class="st"> </span>sim3 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">data_grid</span>(x1, x2) <span class="op">%&gt;%</span><span class="st">   </span>
<span class="st">    </span><span class="kw">gather_predictions</span>(mod1, mod2)
<span class="kw">head</span>(grid, <span class="dv">3</span>)</code></pre></div>
<pre><code>## # A tibble: 3 x 4
##   model    x1 x2     pred
##   &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt;
## 1 mod1      1 a      1.67
## 2 mod1      1 b      4.56
## 3 mod1      1 c      6.48</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(grid, <span class="dv">3</span>)</code></pre></div>
<pre><code>## # A tibble: 3 x 4
##   model    x1 x2      pred
##   &lt;chr&gt; &lt;int&gt; &lt;fct&gt;  &lt;dbl&gt;
## 1 mod2     10 b     -0.162
## 2 mod2     10 c      5.48 
## 3 mod2     10 d      3.98</code></pre>
</div>
<div class="right" ,="" style="width: 50%">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(sim3, <span class="kw">aes</span>(<span class="dt">x=</span>x1, <span class="dt">y=</span>y, <span class="dt">color=</span>x2)) <span class="op">+</span><span class="st">   </span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st">                       </span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data=</span>grid, <span class="kw">aes</span>(<span class="dt">y=</span>pred)) <span class="op">+</span><span class="st">  </span>
<span class="st">    </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>model)                  </code></pre></div>
<p><img src="Lecture6_Data_modeling_and_linear_regression_files/figure-revealjs/unnamed-chunk-29-1.png" width="768" /></p>
</div>
</section><section id="section-3" class="slide level2">
<h2></h2>
<p>Now, we fit <strong>interaction effects</strong> for the <code>mtcars</code> dataset. Note the ‘<code>:</code>’-notation for the interaction term.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mfit_inter &lt;-<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>am <span class="op">*</span><span class="st"> </span>wt, mtcars_train)
<span class="kw">names</span>(<span class="kw">coefficients</span>(mfit_inter))</code></pre></div>
<pre><code>## [1] &quot;(Intercept)&quot; &quot;am&quot;          &quot;wt&quot;          &quot;am:wt&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(mfit_inter)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ am * wt, data = mtcars_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.5603 -1.0064  0.0679  0.7265  3.3565 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  28.7955     3.7796   7.619 6.18e-06 ***
## am           13.7636     4.5621   3.017  0.01072 *  
## wt           -3.3685     0.9759  -3.452  0.00479 ** 
## am:wt        -4.4730     1.3701  -3.265  0.00677 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.721 on 12 degrees of freedom
## Multiple R-squared:  0.9183, Adjusted R-squared:  0.8978 
## F-statistic: 44.94 on 3 and 12 DF,  p-value: 8.43e-07</code></pre>
</section><section id="exercise-1" class="slide level2">
<h2>Exercise 1</h2>
<p></br></p>
<ul>
<li><p>Go to the “Lec6_Exercises.Rmd” file, which can be downloaded from the class website under the Lecture tab.</p></li>
<li><p>Complete Exercise 1.</p></li>
</ul>
</section></section>
<section><section id="lasso-regression" class="titleslide slide level1"><h1>Lasso Regression</h1></section><section id="choosing-a-model" class="slide level2">
<h2>Choosing a model</h2>
<ul>
<li>Modern datasets often have “too” many variables, e.g. predict the risk of a disease from the single nucleotide polymorphisms (SNPs) data.</li>
<li><strong>Issue:</strong> <span class="math inline">\(n \ll p\)</span> i.e. no. of predictors is much larger than than the no. of observations.</li>
<li><strong>Lasso regression</strong> is especially useful for problems, where</li>
</ul>
<blockquote>
<p>the number of available covariates is extremely large, but only a handful of them are relevant for the prediction of the outcome.</p>
</blockquote>
</section><section id="lasso-regression-1" class="slide level2">
<h2>Lasso Regression</h2>
<ul>
<li>Lasso regression is simply regression with <span class="math inline">\(L_1\)</span> penalty.</li>
<li>That is, it solves the problem:</li>
</ul>
<p><span class="math display">\[\boldsymbol{\hat \beta}  = arg \min\limits_{\boldsymbol{\beta}}
\sum_i \left(y^{(i)} 
- \boldsymbol{\beta}^T \boldsymbol{x}^{(i)}\right)^2 + 
\lambda \|\boldsymbol{\beta}\|_1\]</span></p>
<ul>
<li><p>It turns out that the <span class="math inline">\(L_1\)</span> norm <span class="math inline">\(\|\vec \beta\|_1 = \sum_i |beta_i|\)</span> <strong>promotes sparsity</strong>, i.e. only a handful of <span class="math inline">\(\hat\beta_i\)</span> will actually be non-zero.</p></li>
<li><p>The number of non-zero coefficients depends on the choice of the tuning parameter, <span class="math inline">\(\lambda\)</span>. The higher the <span class="math inline">\(\lambda\)</span> the fewer non-zero coefficients.</p></li>
</ul>
</section><section id="glmnet" class="slide level2">
<h2><code>glmnet</code></h2>
<ul>
<li>Lasso regression is implemented in an R package <code>glmnet</code>.</li>
<li>An introductory tutorial to the package can be found <a href="https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html">here</a>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># install.packages(&quot;glmnet&quot;)</span>
<span class="kw">library</span>(glmnet)</code></pre></div>
</section><section id="section-4" class="slide level2">
<h2></h2>
<ul>
<li>We go back to <code>mtcars</code> datasets and use Lasso regression to predict the <code>mpg</code> using all variables.</li>
<li>Lasso will pick a subset of predictors that best predict the <code>mpg</code>.</li>
<li>This means that we technically allow for all variables to be included, but due to penalization, most of the fitted coefficients will be zero.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(mtcars)
<span class="kw">class</span>(mtcars)</code></pre></div>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(mtcars)</code></pre></div>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
</section><section id="fitting-a-sparse-model" class="slide level2">
<h2>Fitting a sparse model</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Convert to &#39;glmnet&#39; required input format:</span>
y &lt;-<span class="st"> </span>mtcars[, <span class="dv">1</span>]  <span class="co"># response vector, &#39;mpg&#39;</span>
X &lt;-<span class="st"> </span>mtcars[, <span class="op">-</span><span class="dv">1</span>] <span class="co"># all other variables treated as predictors</span>
X &lt;-<span class="st"> </span><span class="kw">data.matrix</span>(X, <span class="st">&quot;matrix&quot;</span>) <span class="co"># converts to NUMERIC matrix</span>

<span class="co"># Choose a training set</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
idx &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(mtcars), <span class="kw">floor</span>(<span class="fl">0.7</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(mtcars)))
X_train &lt;-<span class="st"> </span>X[idx, ]; y_train &lt;-<span class="st"> </span>y[idx]
X_test &lt;-<span class="st"> </span>X[<span class="op">-</span>idx, ]; y_test &lt;-<span class="st"> </span>y[<span class="op">-</span>idx]

<span class="co"># Fit a sparse model</span>
fit &lt;-<span class="st"> </span><span class="kw">glmnet</span>(X_train, y_train)
<span class="kw">names</span>(fit)</code></pre></div>
<pre><code>##  [1] &quot;a0&quot;        &quot;beta&quot;      &quot;df&quot;        &quot;dim&quot;       &quot;lambda&quot;   
##  [6] &quot;dev.ratio&quot; &quot;nulldev&quot;   &quot;npasses&quot;   &quot;jerr&quot;      &quot;offset&quot;   
## [11] &quot;call&quot;      &quot;nobs&quot;</code></pre>
</section><section id="section-5" class="slide level2">
<h2></h2>
<ul>
<li><code>glmnet()</code> compute the Lasso regression for a sequence of different tuning parameters, <span class="math inline">\(\lambda\)</span>.</li>
<li>Each row of <code>print(fit)</code> corresponds to a particular <span class="math inline">\(\lambda\)</span> in the sequence.</li>
<li>column <code>Df</code> denotes the number of non-zero coefficients (degrees of freedom),</li>
<li><code>%Dev</code> is the percentage variance explained,</li>
<li><code>Lambda</code> is the value of the currently chosen tuning parameter.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit)</code></pre></div>
<pre><code>## 
## Call:  glmnet(x = X_train, y = y_train) 
## 
##       Df   %Dev   Lambda
##  [1,]  0 0.0000 4.679000
##  [2,]  1 0.1383 4.264000
##  [3,]  2 0.2626 3.885000
##  [4,]  2 0.3700 3.540000
##  [5,]  2 0.4593 3.225000
##  [6,]  2 0.5333 2.939000
##  [7,]  2 0.5948 2.678000
##  [8,]  2 0.6459 2.440000
##  [9,]  2 0.6883 2.223000
## [10,]  2 0.7235 2.026000
## [11,]  2 0.7527 1.846000
## [12,]  2 0.7770 1.682000
## [13,]  3 0.7993 1.532000
## [14,]  3 0.8179 1.396000
## [15,]  3 0.8335 1.272000
## [16,]  3 0.8463 1.159000
## [17,]  3 0.8570 1.056000
## [18,]  3 0.8659 0.962300
## [19,]  3 0.8733 0.876800
## [20,]  4 0.8797 0.798900
## [21,]  4 0.8862 0.727900
## [22,]  4 0.8915 0.663300
## [23,]  4 0.8960 0.604300
## [24,]  4 0.8997 0.550700
## [25,]  4 0.9028 0.501700
## [26,]  4 0.9054 0.457200
## [27,]  4 0.9075 0.416600
## [28,]  4 0.9093 0.379500
## [29,]  5 0.9108 0.345800
## [30,]  6 0.9124 0.315100
## [31,]  5 0.9139 0.287100
## [32,]  5 0.9152 0.261600
## [33,]  5 0.9162 0.238400
## [34,]  5 0.9171 0.217200
## [35,]  5 0.9178 0.197900
## [36,]  5 0.9184 0.180300
## [37,]  5 0.9189 0.164300
## [38,]  5 0.9193 0.149700
## [39,]  4 0.9197 0.136400
## [40,]  4 0.9199 0.124300
## [41,]  4 0.9201 0.113200
## [42,]  4 0.9203 0.103200
## [43,]  5 0.9215 0.094020
## [44,]  7 0.9263 0.085660
## [45,]  7 0.9313 0.078050
## [46,]  6 0.9350 0.071120
## [47,]  6 0.9361 0.064800
## [48,]  6 0.9371 0.059050
## [49,]  7 0.9379 0.053800
## [50,]  7 0.9387 0.049020
## [51,]  8 0.9396 0.044670
## [52,]  9 0.9414 0.040700
## [53,] 10 0.9443 0.037080
## [54,] 10 0.9473 0.033790
## [55,] 10 0.9499 0.030790
## [56,] 10 0.9520 0.028050
## [57,] 10 0.9538 0.025560
## [58,] 10 0.9553 0.023290
## [59,] 10 0.9565 0.021220
## [60,] 10 0.9575 0.019330
## [61,] 10 0.9584 0.017620
## [62,] 10 0.9591 0.016050
## [63,] 10 0.9597 0.014630
## [64,] 10 0.9602 0.013330
## [65,] 10 0.9606 0.012140
## [66,] 10 0.9609 0.011060
## [67,] 10 0.9612 0.010080
## [68,] 10 0.9614 0.009186
## [69,] 10 0.9616 0.008369
## [70,] 10 0.9618 0.007626
## [71,] 10 0.9619 0.006949
## [72,] 10 0.9620 0.006331
## [73,] 10 0.9621 0.005769
## [74,] 10 0.9622 0.005256
## [75,] 10 0.9623 0.004789
## [76,] 10 0.9623 0.004364
## [77,] 10 0.9624 0.003976
## [78,] 10 0.9624 0.003623
## [79,] 10 0.9625 0.003301
## [80,] 10 0.9625 0.003008
## [81,] 10 0.9625 0.002741
## [82,] 10 0.9625 0.002497
## [83,] 10 0.9626 0.002275
## [84,] 10 0.9626 0.002073
## [85,] 10 0.9626 0.001889
## [86,] 10 0.9626 0.001721
## [87,] 10 0.9626 0.001568</code></pre>
</section><section id="section-6" class="slide level2">
<h2></h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># label = TRUE makes the plot annotate the curves with the corresponding coeffients labels.</span>
<span class="kw">plot</span>(fit, <span class="dt">label =</span> <span class="ot">TRUE</span>, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>) </code></pre></div>
<p><img src="Lecture6_Data_modeling_and_linear_regression_files/figure-revealjs/unnamed-chunk-35-1.png" width="960" /></p>
<ul>
<li>the y-axis corresponds the value of the coefficients.</li>
<li>the x-axis is denoted “Log Lambda” corresponds to the value of <span class="math inline">\(\lambda\)</span> parameter penalizing the L1 norm of <span class="math inline">\(\boldsymbol{ \hat \beta}\)</span></li>
</ul>
</section><section id="section-7" class="slide level2">
<h2></h2>
<ul>
<li>Each curve corresponds to a single variable, and shows the value of the coefficient as the tuning parameter varies.</li>
<li><span class="math inline">\(\|\hat \beta\|_{L_1}\)</span> increases and <span class="math inline">\(\lambda\)</span> decreases from left to right.</li>
<li>When <span class="math inline">\(\lambda\)</span> is small (right) there are more non-zero coefficients.</li>
</ul>
<p>The computed Lasso coefficient for a particular choice of <span class="math inline">\(\lambda\)</span> can be printed using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Lambda = 1</span>
<span class="kw">coef</span>(fit, <span class="dt">s =</span> <span class="dv">1</span>)</code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        1
## (Intercept) 34.877093111
## cyl         -0.867649618
## disp         .          
## hp          -0.005778702
## drat         .          
## wt          -2.757808266
## qsec         .          
## vs           .          
## am           .          
## gear         .          
## carb         .</code></pre>
</section><section id="section-8" class="slide level2">
<h2></h2>
<ul>
<li>Like for <code>lm()</code>, we can use a function <code>predict()</code> to predict the <code>mpg</code> for the training or the test data.</li>
<li>However, we need specify the value of <span class="math inline">\(\lambda\)</span> using the argument <code>s</code>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Predict for the test set:</span>
<span class="kw">predict</span>(fit, <span class="dt">newx =</span> X_test, <span class="dt">s =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">1.5</span>, <span class="dv">2</span>))</code></pre></div>
<pre><code>##                           1        2        3
## Datsun 710         25.36098 23.87240 23.22262
## Valiant            19.82245 19.42427 19.41920
## Duster 360         16.19324 17.27111 17.74858
## Merc 230           22.62471 21.86937 21.50396
## Merc 450SE         15.20595 16.16123 16.71324
## Cadillac Fleetwood 11.25687 13.28117 14.26985
## Chrysler Imperial  10.81730 13.01570 14.07314
## Fiat 128           25.88928 24.20103 23.47110
## Toyota Corolla     27.01880 25.08206 24.22690
## Toyota Corona      24.89106 23.51713 22.92237</code></pre>
<p>Each of the columns corresponds to a choice of <span class="math inline">\(\lambda\)</span>.</p>
</section><section id="choosing-lambda" class="slide level2">
<h2>Choosing <span class="math inline">\(\lambda\)</span></h2>
<ul>
<li>To choose <span class="math inline">\(\lambda\)</span> can use <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross-validation</a>.</li>
<li>Use <code>cv.glmnet()</code> function to perform a k-fold cross validation.</li>
</ul>
<blockquote>
<p>In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. <a href="#/fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
</blockquote>
</section><section id="section-9" class="slide level2">
<h2></h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
<span class="co"># `nfolds` argument sets the number of folds (k).</span>
cvfit &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(X_train, y_train, <span class="dt">nfolds =</span> <span class="dv">5</span>)
<span class="kw">plot</span>(cvfit)</code></pre></div>
<p><img src="Lecture6_Data_modeling_and_linear_regression_files/figure-revealjs/unnamed-chunk-38-1.png" width="960" /></p>
<ul>
<li>The <span style="color:red">red dots</span> are the average MSE over the k-folds.</li>
<li>The two chosen <span class="math inline">\(\lambda\)</span> values are the one with <span class="math inline">\(MSE_{min}\)</span> and one with <span class="math inline">\(MSE_{min} + sd_{min}\)</span></li>
</ul>
</section><section id="section-10" class="slide level2">
<h2></h2>
<p><span class="math inline">\(\lambda\)</span> with minimum mean squared error, MSE:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvfit<span class="op">$</span>lambda.min</code></pre></div>
<pre><code>## [1] 0.2171905</code></pre>
<p>The “best” <span class="math inline">\(\lambda\)</span> in a practical sense is usually chosen to be the biggest <span class="math inline">\(\lambda\)</span> whose MSE is within one standard error of the minimum MSE.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvfit<span class="op">$</span>lambda.1se</code></pre></div>
<pre><code>## [1] 0.6632685</code></pre>
<p>Predictions using the “best” <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">final_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(cvfit, <span class="dt">newx=</span>X_test, <span class="dt">s=</span><span class="st">&quot;lambda.1se&quot;</span>)
final_pred</code></pre></div>
<pre><code>##                           1
## Datsun 710         25.01062
## Valiant            19.68422
## Duster 360         16.32664
## Merc 230           22.44375
## Merc 450SE         15.35370
## Cadillac Fleetwood 11.58909
## Chrysler Imperial  11.13782
## Fiat 128           25.54984
## Toyota Corolla     26.64431
## Toyota Corona      24.55160</code></pre>
</section></section>
<section><section id="more-on-models" class="titleslide slide level1"><h1>More on models</h1></section><section id="building-models" class="slide level2">
<h2>Building Models</h2>
<p>Building models is an important part of EDA.</p>
<p>It takes practice to gain an intuition for which patterns to look for and what predictors to select that are likely to have an important effect.</p>
<p>You should go over examples in <a href="http://r4ds.had.co.nz/model-building.html" class="uri">http://r4ds.had.co.nz/model-building.html</a> to see concrete examples of how a model is built for <code>diamonds</code> and <code>nycflights2013</code> datasets we have seen before.</p>
</section><section id="other-model-families" class="slide level2">
<h2>Other model families</h2>
<p>This chapter has focused exclusively on the class of linear models <span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon =
\vec \beta \vec x + \epsilon
\]</span></p>
<p>and penalized linear models.</p>
<p>There are a large set of other model classes.</p>
</section><section id="section-11" class="slide level2">
<h2></h2>
<p>Extensions of linear models:</p>
<ul>
<li>Generalized linear models, <code>stats::glm()</code>, binary or count data.</li>
<li>Generalized additive models, <code>mgcv::gam()</code>, extend generalized linear models to incorporate arbitrary smooth functions.</li>
<li>Robust linear models, <code>MASS:rlm()</code>, less sensitive to outliers.</li>
</ul>
<p>Completely different models:</p>
<ul>
<li>Trees, <code>rpart::rpart()</code>, fit a piece-wise constant model splitting the data into progressively smaller and smaller pieces.</li>
<li>Random forests, <code>randomForest::randomForest()</code>, aggregate many different trees.</li>
<li>Gradient boosting machines, <code>xgboost::xgboost()</code>, aggregate trees.</li>
</ul>
</section><section id="useful-books" class="slide level2">
<h2>Useful Books</h2>
<ul>
<li><p><a href="http://www-bcf.usc.edu/~gareth/ISL/getbook.html">“An introduction to Statistical Learning”</a> [ISL] by James, Witten, Hastie and Tibshirani</p></li>
<li><p><a href="http://www.springer.com/gp/book/9780387848570">“Elements of statistical learning”</a> [ESL] by Hastie, Tibshirani and Friedman</p></li>
<li><p><a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470542810.html">“Introduction to Linear Regression Analysis”</a> by Montgomery, Peck, Vinning</p></li>
</ul>
</section></section>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation" class="uri">https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation</a><a href="#/fnref1">↩</a></p></li>
</ol>
</section>
    </div>
  </div>

  <script src="libs/reveal.js-3.3.0.1/lib/js/head.min.js"></script>
  <script src="libs/reveal.js-3.3.0.1/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display the page number of the current slide
        slideNumber: true,
        // Push each slide change to the browser history
        history: true,
        // Vertical centering of slides
        center: true,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition style for full page slide backgrounds
        backgroundTransition: 'default', // none/fade/slide/convex/concave/zoom



        // Optional reveal.js plugins
        dependencies: [
        ]
      });
    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
