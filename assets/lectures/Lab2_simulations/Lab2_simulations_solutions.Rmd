---
title: "Lab 7: Simulations Answers"
author: "Week 3 Session 7"
date: "07/13/2020"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
set.seed(447872364)
```

## Quiz Answers

**Quiz question 1** : What happens if you replace ``numeric(1)`` in the above 
code with ``numeric(2)``?

**Answer** : This would return an error because the function 
`function(i) { sum(rexp(n = nexps, rate = rate))}` returns a single
(length = 1) numeric value.

**Quiz question 2** : What if you replace ``numeric(1)`` by ``character(1)``?

**Answer** : This again returns an error as
`function(i) { sum(rexp(n = nexps, rate = rate))}` returns a numeric value
and not a character value.

**Quiz question 3** : What is the most common outcome of a Binomial 
distribution with parameters `prob=0.4` and `size=20`?

**Answer** : The most common outcome is 8.

Simulation based answer:
```{r}
num_trials <- 10000
binom_variable <- rbinom(n = num_trials, prob = 0.4, size = 20)
freqs <- table(binom_variable)
freqs
names(freqs)[which.max(freqs)]
```

Distribution based answer:
```{r}
x <- 1:20
# Probability Mass Function (discrete version of a pdf)
pmf <- dbinom(x, prob = 0.4, size = 20)
# Plot the distribution of Binomial Variable with specified parameters
# Then, look for the x-value which hives the highest probability density
plot(x = x, y = pmf)
```

**Quiz question 4** : What is the probability of the most common outcome of a Binomial distribution with parameters `prob=0.4` and `size=20`?

**Answer** : The probability of the most common outcome is around 0.1797058.

```{r}
dbinom(x = 8, prob = 0.4, size = 20)
```


**Quiz question 5** : What is the most common outcome for a Geometric 
distribution with parameter `prob=0.2`?

**Answer** : The most common outcome is 0.

Simulation based answer:
```{r}
num_trials <- 10000
binom_variable <- rgeom(n = num_trials, prob = 0.2)
freqs <- table(binom_variable)
freqs
names(freqs)[which.max(freqs)]
```

Distribution based answer:

```{r}
x <- 0:10
pdf <- dgeom(x, prob = 0.2)
# Plot the distribution of Binomial Variable with specified parameters
plot(x = x, y = pdf)
```


**Quiz question 6** : What is the smallest $n$ such that a random variable drawn 
from a Geometric distribution with parameter `prob=0.2` is less or equal to $n$ 
with probability greater or equal to $0.95$?

**Answer** : The smallest $n$ is 13.

```{r}
ns_cutoff <- 0:100 #100 arbitrary, large number
ns_cutoff[min( which(pgeom(ns_cutoff, prob=0.2) >= 0.95))]
```

An alternative way to arrive at the same result is using the `qgeom` function:

```{r}
qgeom(p = 0.95, prob = 0.2)
```


**Quiz question 7** : What is the most common outcome for a Geometric distribution with parameter `prob=0.8`?

**Answer** : The most common outcome is 0.

Simulation based answer:
```{r}
num_trials <- 10000
binom_variable <- rgeom(n = num_trials, prob = 0.8)
freqs <- table(binom_variable)
freqs
names(freqs)[which.max(freqs)]
``` 


Distribution based answer:

```{r}
x <- 0:10
pdf <- dgeom(x, prob = 0.8)
# Plot the distribution of Binomial Variable with specified parameters
plot(x = x, y = pdf)
```


**Quiz question 8** : What is the smallest $n$ such that a random variable
drawn from a Geometric distribution with parameter `prob=0.8` is less or
equal to $n$ with probability greater or equal to $0.95$?

**Answer** : The smallest $n$ is 1.

```{r}
ns_cutoff <- 0:100 #100 arbitrary, large number
ns_cutoff[min( which(pgeom(ns_cutoff, prob=0.8) >= 0.95))]
```

Using `qgeom`:

```{r}
qgeom(p = 0.95, prob = 0.8)
```

**Quiz question 9** : Find by simulations using the `var` function what the 
variance of the Poisson distribution with parameter `lambda = 5` is 
( +/- 0.3 deviation accepted). For example you might want to generate
1000 instances of a Poisson(5) random variable.

**Answer** : The variance is about 5.

```{r}
num_trials <- 50000
var(rpois(n = num_trials, lambda = 5))
```

Note that the theoretical variance of Poisson variables is equal to lambda
parameter, so we got a consistent answer.

**Quiz question 10** : Now instead consider the sum of 50 independent Poisson
(0.1) random variables. Again using simulation (or otherwise), find the variance 
of this (again within +/- 0.3 range accepted).

**Answer** : The variance is about 5.

```{r}
num_trials <- 50000
var(replicate(num_trials, sum(rpois(n = 50, lambda = 0.1))))
```

Note, that we again get the same answer. This is because, a sum of Poisson
variables, $X_i \sim Pois(\lambda_i), \quad i = 1, \dots, M$  is Poisson 
distributed $Y = \sum_{i = 1}^M X_i \sim Pois(\sum_{i = 1}^M\lambda_i)$


**Quiz Question 11** : How many chromosomes are stored in `Celegans`?

**Answer **: There are 7 chromosomes
```{r warning=FALSE, message=FALSE}
library("BSgenome.Celegans.UCSC.ce2")
library("Biostrings")
Celegans
```

```{r}
length(Celegans)
```


**Quiz Question 12** : What is the length of all chromosomes stored in `Celegans` 
combined?  (Hint: Look at the documentation of the ``BSgenome`` class.)

**Answer** :  Combined length is 100291769.

```{r celegans-length, exercise=TRUE}
sum(seqlengths(Celegans))
```

**Quiz Question 13** : Which is the smallest chromosome of `Celegans`?

**Answer** : The smallest is the chromosome chrM.

```{r}
which.min(seqlengths(Celegans))
```

**Quiz Question 14** : Based on these results, do you believe that the 
nucleotide frequencies in the _C.elegans_ chrM sequence are all the same? 



```{r}
# nucleotide frequency of chrM
lfM = letterFrequency(Celegans$chrM, letters=c("A", "C", "G", "T"))
lfM
```

We're going to compute a statistic that measures how close two multionial 
outputs are to each other as sum of squared deviation from expected 
frequencies over expected frequencies. We'll take the average squared 
difference between expected (`e`) and observed (`o`) counts, scaled by `e`.
We will call the function `oestat`:

```{r celegans-oestat}
oestat = function(o, e){
  sum( (e-o)^2/e )
}
n = length(Celegans$chrM)
expected = rep(n/4 ,4)
oe = oestat( o = lfM, e=expected)
oe
```

Then, we compute the distribution of the same statistics assuming it
came from the null distribution if all nucleotides were equally likely:

```{r, celegans-replicate}
set.seed(1)
B = 10000
oenull = replicate(B, oestat(e=expected, o=rmultinom(1,size = n, prob = rep(1/4,4))))
```


```{r}
hist(oenull, breaks = 50)
```

```{r}
quantile(oenull, p = 0.95)
oe
oe > quantile(oenull, p = 0.95)
```


**Answer** : The observed statistic 'oe' is much bigger than the
95% quantile of the empirical null distribution of the statistics,
so we can conclude that Celegans's chrM does not have an even distribution
of the nucleotides.



## In Class Exercices

1. You want to invest in Netflix stock options. You know that the log return  follows a normal distribution, such that:
$$ \log(\frac{p_{t+1}}{p_t})  \sim N(0.001, 0.01)$$

a. If you invest, what is the probability of the price doubling over the next hundred days?

__ First solution__

$$ \log(\frac{p_{100}}{p_0}) = \sum_{t=1}^{100}\log(\frac{p_{t}}{p_{t-1}})   \sim N(0.1, 1)$$
$$\implies  \frac{p_{100}}{p_0} \geq 2 \iff N(0.1,1) \geq log(2) =0.69 $$

```{r}
1-pnorm(0.69, mean=0.1,sd=1)
```


__Simulation based solution__

$$ \log(p_t) = N(0.001 + log(p_t), 0.01)$$

```{r}
B = 1000
price = matrix(0, B, 100+1)
price[,1] =1
for(t in 1:100){
  price[,t+1] = sapply(price[,t], function(x){exp(rnorm(1, 0.001 + log(x),sqrt(0.01)))})
}

```

```{r}
mean(price[,101]>=2)
```


b. How long does it typically take for the price to double?

```{r}
B = 1000
T = apply(price,1, function(x){ifelse(max(x)>2, which(x>=2)[1], 0)})
print(mean(T[T>0]))
```


c. Imagine a strategy where at day 1,  you invest 100 dollars, buying 100 options at 1 dollar. You then cash out whenever your investment doubles, and re-invest 100 dollars. At the end of the 100 days, you sell what you have at the current price. What is your expected profit after 100 days? What is the probability that the profit exceeds 2,000 dollars? The probability that you loose money?

```{r}
profit = apply(price, 1,function(x){
  t = floor(max(log(x)/log(2)))
  if (t>=1){
    100* floor(max(x)/2) + x[101] * 100/x[which(x>=exp(t*log(2)))[1]]
  }else{
    - 100 + x[101] * 100/1
  }
})

print(mean(profit))
print(mean(profit>2000))
print(mean(profit<0))
```


2. Your friend in the department of psychology is running an experiment to assess the negative effect of sugar on memory task. For that purpose, he wants to find two (equal) groups of people, who will be asked to remember a series of associations, one, after drinking a sugar-heavy ice-tea, and the other, after drinking a ``diet'' version of the previous with synthetic sugar. He supposes that the range of scores (from 0 to 50) will be normally distributed for each group and wants to detect a difference of at least 2 points. Preliminary results indicate that the standard deviation within the groups is roughly 5 points, and the mean for the control is roughly 60. How many candidates should he pick to be 95\% sure that there is a difference? In other words, how many candidates should he pick to ensure that the difference in means in greater than what chance would yield in 95\% of cases if the two groups were identically distributed?


```{r}
B =5000
power_computation =function(x){
  thres = quantile(sapply(1:10000, function(y){mean(rnorm(x, 60,5))  - mean(rnorm(x, 60,5))} ),0.95)
  diff = sapply(1:B, function(y){
      G1= rnorm(x, 60,5)
      G2 = rnorm(x, 58,5)
      return(mean(G1) - mean(G2)> thres)
      #return(mean(G1) - mean(G2)> qnorm(0.95, 0, 2*5/sqrt(x))) ### simpler way
  })
  return(mean(diff))
}
power = sapply(seq(1,200,2), power_computation )
plot(seq(1,200,2), power)
abline(v = seq(1,200,2)[which(power > 0.95)[1]])
```






