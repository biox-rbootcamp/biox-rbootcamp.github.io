---
title: "Session 18 In-Class Exercise Solution"
author: "BioX R Bootcamp"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
urlcolor: blue
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache=FALSE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE, 
  results = 'markup', dev='png', dpi=150
)
```

## Simple Linear Regression

Download the dataset meap93.Rdata. Take $y$ to  be  the  variable `math10` which denotes  the  percentage of tenth graders at  a  high  school receiveing a passing score on a standardized mathematics exam.  Take $x$ to be the variable `lnchprg` which  denotes  the  percentage  of  students who  are  eligible  for  a  federally funded school lunch program.

```{r}
load("meap93.Rdata") # the loaded dataset will be called `data`
```

a) Fit a simple linear regression model for $y$ on $x$.  Report the estimates of $\beta_0$ and $\beta_1$ together with their standard errors. 

```{r}
model<-lm(math10~lnchprg, data = data)
summary(model)$coefficients
```

b) We would expect the lunch program to have a positive effect on student performance.  Does your model support such a positive relationship?  If yes, explainwhy.  If no, what went wrong?

**Answer:** No, the coefficient of the lunch program variable is negative, indicating a negative effect on student performance.This could be because we are not accounting for other variables that influence math scores.

**Now we want to explore the relationship between the math pass rate (`math10`) and spending per student (`expend`).**

c) Do you think each additional dollar spent has the same effect on the pass rate, or does a diminishing effect seem more appropriate?  Explain. 

**Answer:** A diminishing effect seems more appropriate. Increasing the spending in a low intial budget would have moreimpact than increasing the spending in a budget where spending is already high. (If the spending is alreadyhigh, students likely have already have access to useful resources, and increasing the spending would probablynot provide much additional value to students that would increase their math pass rate.)

d) In the model $\text{math10} = \beta_0+ \beta_1 log(\text{expend}) +e$, argue that $\beta_1/10$ is the percentage point change in `math10` given a 10 percent increase in expend.

**Answer** Let $y$ represent the `math10` variable and $x$ represent the `expend` variable. We are interested in the difference $y_2-y_1$, where

$$ y_1 = \beta_0 + \beta_1 log(x_1) + e$$

$$ y_2 = \beta_0 + \beta_1 log(x_1 * 1.10) + e$$

Taking the difference, $$y_2-y_1 = \beta_1 log(1.10) \approx \frac{\beta_1}{10}$$

e) Use the data to estimate the parameters $\beta_0$ and $\beta_1$ in the above model. Report the estimates and standard errors. 

```{r}
model<-lm(math10~lexpend, data = data)
summary(model)$coefficients
```


f) How big is the estimated spending effect i.e., if spending increases by 10 percent,what is the estimated percentage point increase in `math10`?

**Answer:** If spending increases by 10 percent, `math10` will increase by an estimated 1.1164395 percentage points.

g) One might worry that regression analysis can produce fitted values for `math10` that are greater than 100. Why is this not much of a worry in this dataset? (Hint: can use a histogram to plot `hist()`)

**Answer:** In this dataset, the `math10` scores are all much less than 100: they are centered around 24 with a maximum value of 66.7. Thus, our model is designed to measure the effects of variables within this range of $y$ values. Since our model is based on this lower range of $y$ values, we have no reason to believe that our model would be sensible for predictions of $y$ that are much higher than this range.

```{r}
hist(data$math10)
```

## Multiple Regression

Let's reconsider the cross-sectional wage data sampled from the 1976 US Population Survey[^1] that we explored last time. It consists of 526 observations of average hourly wages, and 
various covariates such as education, race, gender, or marital status. 

[^1]: Wooldridge, J.~M. (2000), Introductory Econometrics: A Modern Approach, South-Western College Publishing.

### Loading the wages dataset
```{r}
# uncomment the install.packages if you do not have this package
#install.packages('wooldridge')
require('wooldridge')
data('wage1')
head(wage1)
```

As a refresher, let's look at the distribution of wages, in dollars per hour. Recall that if we subset the data by gender, we get two histograms that look different. This suggests that gender likely has an effect on wage, and we should include it in our regression analysis. 
```{r}
library(dplyr)
library(ggplot2)
# A histogram of wages
wage1 %>% ggplot() + 
  geom_histogram(aes(x = wage), 
                  fill = 'light blue', 
                  color = 'blue')

# we can color by gender, for example
# ggplot is great for making clear labels and customizing plots with pretty colors
wage1 %>% mutate(gender = as.factor(female)) %>%
  ggplot() + 
  geom_histogram(aes(x = wage,fill = gender), position = "dodge") +
  scale_fill_manual(labels = c("Men", "Women"), values = c("seagreen", "orchid")) 

# we could also just look at simple boxplots:
ggplot(wage1) + geom_boxplot(aes(x=factor(female), y = log(wage))) +
  labs(x = "Gender") + scale_x_discrete(labels = c("Men", "Women"))
```

### Multiple regression model
In the previous lab, we ignored the gender variable and predicted wage only by education.
Why might this have been a problem?

Multiple linear regression allows us to include multiple variables in our linear model. But which variables should we choose? This is a difficult question that we will discuss later in the course. For now, let's visually explore the effect of some variables.

Pick a few variables that you think might affect wages, and make some plots that would visually display the effect of this variable on wage. (Hint for making better plots: last time, we discussed that taking the log of wage was a good idea.)

The variables in the dataset are:

* wage                     average hourly earnings
* educ                     years of education
* exper                    years potential experience
* tenure                   years with current employer
* nonwhite                 =1 if nonwhite
* female                   =1 if female
* married                  =1 if married
* numdep                   number of dependents
* smsa                     =1 if live in SMSA
* northcen                 =1 if live in north central U.S
* south                    =1 if live in southern region
* west                     =1 if live in western region
* construc                 =1 if work in construc. indus.
* ndurman                  =1 if in nondur. manuf. indus.
* trcommpu                 =1 if in trans, commun, pub ut
* trade                    =1 if in wholesale or retail
* services                 =1 if in services indus.
* profserv                 =1 if in prof. serv. indus.
* profocc                  =1 if in profess. occupation
* clerocc                  =1 if in clerical occupation
* servocc                  =1 if in service occupation
* lwage                    log(wage)
* expersq                  exper^2
* tenursq                  tenure^2

```{r}
# Make plots examining the effect of some other variables (besides education) on wage

# Example 1: Scatterplots (for continuous variables)
ggplot(wage1) + geom_point(aes(x=exper, y = log(wage)))
ggplot(wage1) + geom_point(aes(x=tenure, y = log(wage)))

# Example 2: Boxplots (for categorical variables)
ggplot(wage1) + geom_boxplot(aes(x=factor(married), y = log(wage)))
ggplot(wage1) + geom_boxplot(aes(x=factor(profocc), y = log(wage)))
```

Let's create a model that examines the effects of gender, education, and tenure on (log) wages. What is the regression model?
Fit this model and print the coefficients: 
```{r}
# fit a multiple linear regression of wage on gender, education, and tenure:
model<- lm(log(wage) ~ female + educ + tenure, data = wage1)
summary(model)
```
What are the estimated coefficients? What do they mean? 
How is the interpretation different from simple linear regression?

### Dummy variables
Gender is a categorial variable: it is not a numeric observation like years of education. How can we use this variable in a linear model? 

A *dummy variable*, or *indicator varible*, is used to represent the levels of a categorical variable. 
In this dataset, we can see that the gender variable has already been transformed into a dummy variable:

```{r}
head(wage1)
table(wage1$female)
```

In R, we could fit a model by first transforming any categorical variable into a dummy variable, like the wage dataset has done. This is not necessary, however. We could just as easily use a factor variable in the lm() function.
```{r}
# let's add a categorial variable to indicate gender rather than using the dummy variable
wage1$gender<- wage1$female
wage1$gender[wage1$gender == 1]<- "women"
wage1$gender[wage1$gender == 0]<- "men"
wage1$gender<- factor(wage1$gender) # now we have gender stored as a factor variable
table(wage1$gender)
lm(log(wage) ~ educ + gender, data = wage1) # factor variable
lm(log(wage) ~ educ + female, data = wage1) # dummy variable
```

So if we give lm() a categorical variable, lm() will transform this into a dummy variable behind the scenes so that it can do the mathematical computation of fitting the model.

What happens if we have more than two categories for a categorical variable? For example, we might be interested in how the region of the country affects wages. We could collect this information in our dataset with a variable *region* with categories north/central, south, west, and other. How can we make a dummy variable for this variable?

One idea might be to just use values of $0, 1, 2, 3$ to represent the different regions. Are there problems with this approach?

The problem here is that using non-binary numbers imposes an ordering on the categories of our regions. By giving the regions numeric values, we are essentially saying that wages for the north will be lower than the south, which are lower than the west, which are lower than the other category. We do not want to impose this sort of ordering on the variable.

The solution is to create multiple binary dummy variables: specifically, one dummy variable for whether or not the region is "south", another dummy variable for whether or not the region is "west", and one dummy variable for whether or not the region is "north/central". 
Note that we only need 3 dummy variables. (Why?)

The wage dataset is actually already set up with dummy variables. 
Let's imagine that we had a categorical variable region to see how the dummy variables correspond to the region variable.
```{r}
# make a categorical variable region
wage1$region<- "other"
wage1$region[wage1$northcen == 1]<- "northcen"
wage1$region[wage1$south == 1]<- "south"
wage1$region[wage1$west == 1]<- "west"
wage1$region<- factor(wage1$region, levels = c("other", "northcen", "south", "west"))

# recording the data as a categorical variable makes plotting multiple categories simpler
ggplot(wage1) + geom_boxplot(aes(x=region, y = log(wage)))

# take a look at how the dummy variables correspond to the region variable
wage1[490:515,c("wage", "northcen", "south", "west", "region")]
```
What do the dummy variables mean here? How do they correspond to the region variable?

Now if we wanted to include region in our model, we could use the region variable, and the lm() function would create its own dummy variables, just like the wage dataset has already done.
Fit a linear model of log wages by education and region. Compare the results if you give lm() region as a categorial variable versus if you give lm() the dummy variables. Are they the same?
```{r}
# fit a linear model of education by region,
# first by using the factor variable region:
lm(log(wage) ~ educ + region, data = wage1)
# next by using the dummy variables for region:
lm(log(wage) ~ educ + northcen + south + west, data = wage1)

# in both cases, R constructs the design matrix in the same way:
head(model.matrix(~educ + region, data = wage1))
head(model.matrix(~educ + northcen + south + west, data = wage1))
```

In summary, if we give the lm() function a categorial variable, it will create dummy variables as shown above to represent the categorical variable.