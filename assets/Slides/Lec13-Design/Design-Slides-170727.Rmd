---
title: "Some pointers to statistical analysis of Microbiome Data in R"
author: Susan Holmes
date: 2017-07-27
output: 
  slidy_presentation:
    font_adjustment: +1
bibliography: ../F1000WF.bib
---

<link href='http://fonts.googleapis.com/css?family=Lato&subset=latin,latin-ext' rel='stylesheet' type='text/css'>

<style type="text/css"> 
.small-code pre code {
  font-size: 1em;
}
body{
  font-family: Lato;
  font-size: 24pt;
}
h1{
  font-family: Lato;
  font-size: 32pt;
  color: #0080FF;
}
h2,h3,h4,h5,h6{
  font-family: Lato;
  font-size: 24pt;
  color: #0080FF;
}
</style>

```{r initialize, echo = FALSE, warning = FALSE}
## Set default options for code chunks
knitr::opts_chunk$set(tidy = FALSE, cache = TRUE, autodep = TRUE,
     dev = "png", dpi = 300,
     size = "small",
     message = FALSE, error = FALSE, warning = TRUE)

## output width
base::options(width = 70,  continue = " ")

## set seed - to have reproducibility
set.seed(0xbedada)
```

# Goals for this Lecture

-   Phyloseq
-   Data integration: DPCoA
-   Transformations, filtering
-   Types of experiments, studies, ...
-   Testing tools: networks, DESeq2, 
-   Reproducible Research: available Rmd files

# Phyloseq




<div style="text-align: center;"> ![Tosser](../../images/tosser3.png){width="50%"} 
Tosser </div>


<div style="text-align: center;"> 
![](.png){width="50%"}
</div>


</div>

# Basic principles in the design of experiments

- Balancing.
- Avoid confounding.
- Blocking.
- Randomization.
- Attention to detail.

# Clever combinations and balancing: a motivating example

![Weighing example](../../images/balancechem.png){width="20%"} | A pharmacist's balance weighing analogy (@Hotelling1944 and @Mood1946). 
----------------:|:---------------------------

Experimental design aims to maximize the available resources:
capitalize on cancellations and symmetries 

Hotelling devised an improved weighing scheme using experimental design.

Given a set of eight objects of unknown weights
$m=(m_1,\ldots,m_8)$.

For our experiment, we create a true $m$ randomly.

```{r HotellingsExpt}
set.seed(0xdada2)
m = sample(seq(2, 16, by = 2), 8)  + round(rnorm(8, 1), 1)
m
```

## Method 1: Naive method, eight weighings

Suppose we our scale has errors
distributed normally with a SD of 0.1. 

First, let's weighs each $m_i$ individually 

We simulate the measurements (incl. individual errors) as follows.
We also compute their overall variance as a measure of overall error.

```{r SimpleWeighing}
X_1to8 = m + rnorm(length(m), mean = 0, sd = 0.1)
X_1to8
errors_1to8 = X_1to8 - m
errors_1to8
var(errors_1to8)
```

## Method 2: Hotelling's method, eight weighings

```{r HotellingsMethod, message = FALSE, warning = FALSE}
library("survey")
h8 = hadamard(6)
coef8 = 2 * h8 - 1
coef8
```

We use the columns of this matrix to guide our new weighing scheme.

The first column says we should put all 8 pieces on one side of the balance and weighs them, call this
$Y[1]$. 

The second column says: place 
`r paste(which(h8[, 2] > 0), collapse = ", ")` on one side of the balance and 
`r paste(which(h8[, 2] < 0), collapse = ", ")` on the other and evaluate the difference. 
Call the results $Y[2]$.

And so on.

Each of the eight weighings has a normal error with sd = 0.1.

```{r}
Y = m  %*% coef8 + rnorm(length(m), mean = 0, sd = 0.1)
mhat = Y %*% t(coef8) / 8
```

Now, because in this case we know the true $m$s we can compute the
errors and their variance:

```{r Hoterrors}
errors_hotel = as.vector(mhat) - m
errors_hotel
var(errors_hotel)
var(errors_1to8) / var(errors_hotel)
```


# Experiment by simulation
We saw that the second method had variance almost an order of magnitude smaller than the first. Were we just lucky?

- Repeat Hotelling's weighing experiment B = 10,000 times with different
$m$ and look at the sampling distributions of the variances of the
errors in both schemes

- Guess at the relationship between the two variances.

```{r bootstrapHotelling, message=FALSE, warning=FALSE, fig.width = 2.7, fig.height = 2.7}
tcoef8 = t(coef8) / 8
errs = replicate(10000, {
  m = sample(seq(2, 16, by = 2), 8) + round(rnorm(8, 1), 1)
  X_1to8 = m + rnorm(length(m), mean = 0, sd = 0.1)
  err_1to8 = X_1to8 - m
  Y = coef8 %*% m + rnorm(length(m), mean = 0, sd = 0.1)
  mhat = tcoef8 %*% Y
  err_hotel = mhat - m
  c(var(err_1to8), var(err_hotel))  
})

library("ggplot2")
library("tibble")
ggplot(tibble(ratio = errs[1,] / errs[2,]), aes(x = log10(ratio))) +
  geom_histogram(bins =  50)
```

We say that the second scheme is more efficient than the
first by a factor of 8 because the errors generated by the measurement
have a variance which is 8 times lower. 

# One factor at a time?

Ibn Sina  (Avicenna)  | | Physician Scientist
-------------------:|:----------------:|:-------------------        
![Avicenna](../../images/Avicenna.png){width="100%"} | ![The Physician](../figures/medicus.jpeg){width="100%"} | His *Canon of Medicine (1020)*  lists seven rules of experimental design, including the need for controls and replication, the danger of confounding and the necessity of changing only one factor at a time 

## This dogma was overthrown in the 20th century by RA Fisher.


# Comparing two levels of one factor: healthy or diseased.

```{r illustrateboxplots, echo=FALSE,warning=FALSE,message=FALSE}
require(gridExtra)
library(ggplot2)
library(ggbeeswarm)
library(dplyr)
set.seed(3141221)
states=factor(c("healthy","tumor")[rep(c(1,2),6)])
times=c(1,1,1,1,2,2,2,2,3,3,3,3)
facte  = 0.5*times
exprst = facte+rep(c(0.5,1),6)+rnorm(12,0,0.3)
exprs0  = rep(c(1.5,2),6)+rnorm(12,0,0.1)
batch = factor(c("Batch1","Batch2")[rep(c(1,2),6)])
stateN=factor(c("healthy","tumor")[rep(c(1,2),60)], levels=c("healthy","tumor"))
exprsN  = rep(c(1.5,2),60)+rnorm(120,0,0.3)
dfN=data.frame(stateN,exprsN)
mN=summarise(group_by(dfN,stateN),med=median(exprsN))
pN= ggplot(dfN, aes(x=stateN, y=exprsN))

pN1= pN+geom_boxplot(alpha=0.5,col="blue") +
      geom_point(size=2,alpha=0.5) +
  geom_segment(data=mN,aes(y=med[1],yend=med[2]),x=1.5,xend=1.5, col="red",arrow = arrow(length = unit(0.5, "cm"), ends = "both", type = "closed"))
   

pNb=pN+geom_boxplot(alpha=0.5,col="blue")+
      geom_beeswarm(size=2,alpha=0.5)


batdef=data.frame(states,exprst,exprs0,times,batch)
ms0=summarize(group_by(batdef,states),y=median(exprs0))
p0= ggplot(batdef, aes(x=states, y=exprs0))
p0= p0+geom_boxplot(alpha=0.5,col="blue") +
  geom_point(size=2)
p0batch= ggplot(batdef, aes(x=states, y=exprs0,color=batch))
p0batch= p0batch+geom_boxplot(alpha=0.5,col="blue") +
  geom_point(size=2)
p0effect= p0 + geom_segment(data=ms0,aes(y=ms0[1,2],yend=ms0[2,2]),x=1.5,xend=1.5, col="red",arrow = arrow(length = unit(0.5, "cm"), ends = "both", type = "closed"))


p = ggplot(batdef, aes(states, exprst, times))
p2= p + geom_boxplot() +  
  geom_point(color=times,size=2,alpha=0.5)
p1 = p + geom_boxplot(alpha=0.5,col="blue") +  
  geom_point(size=2,alpha=0.5)
```

```{r }
grid.arrange(p0,p0effect, ncol=2)
```

However if we color according to the batches:
```{r}
grid.arrange(p0,p0batch, ncol=2)
```

We cannot conclude because we are in the presence of **confounding**.



An experiment with higher noise levels, same number of points as in the previous study 
sample sizes (2 x 6) is not enough, but with the same error and a bigger sample: 
 (2 x 60).

```{r}
grid.arrange(p1,pN1, ncol=2)
```

The experiment at $n_1=n_2=6$ is not **powerful** enough.

```{r}
t.test(exprst~states,data=batdef)
```

With the same effect size and a larger sample size,
we have the **power** to see the difference:

```{r}
t.test(exprsN~stateN,data=dfN)
```

# Summarize what we see in these boxplots.

 - Success of an experiment: seeing a true difference that is there.
 - Knowing where the difference comes from.      
 - True positive rate= power.
 - Variances (se) matter as much as the point estimates.
 
Depends on: 

 - 1) Effect size (**unchangeable**)
 - 2) Control and documentation of all factors (block effects, date/time/operator effects).
 - 3) Sample size: remember 
      $$\mbox{standard deviation(mean)} = \frac{1}{n} \sigma^2$$
 - 4) Noise (variability in measurements)
 

# Theory of multifactorial experiments.

Useful language

 - Power is the true positive rate
 $$P(\mbox{see a difference } \;\; | \;\; \mbox{ there is a difference})$$
 - Factors.
 - Experimental units (sole, soil, fish examples)
 - Treatment, 
 - Standard deviation, standard error.
 - Effects, fixed and random.
 - Interaction (won't have time to talk about this).
 
 
# Decomposition of variability: analysis of variance.

```{r block,echo=FALSE} 
grid.arrange(p1,p2, ncol=2)
``` 



# Blocking : the case of paired experiments.


![ZeaMays](../../images/maizeDarwin.png){width="50%"}

Each pot in Darwin's *Zea Mays* experiment is a block, only
the factor of interest should be different (pollination method), all
other factors should be kept equal within a block.

A balanced design is an experimental design where all the different
factor combinations have the same number of observation replicates. 
Such
data are particularly easy to analyse because the effect of each factor
is identifiable. 

When there are (likely) nuisance factors, it is good to
make sure they are balanced with the factors of interest. Sometimes this
is inconvenient or impractical for logistic or economic reasons -- but in
such cases analysts are on thin ice and need to proceed with caution.


# Comparing paired versus unpaired design

When comparing various possible designs, we do power
simulations. 

Here,
we suppose the sample size is 15 in each group and the <span>effect
size</span> is 0.2. We also need to make assumptions about the standard
deviations of the measurements, here we suppose both groups have the
same sd=0.25.

```{r zeamays}
set.seed(45123)
n=15; effect=0.2
pots=rnorm(15,0,1)
noiseh=rnorm(15,0,0.25)
noisea=rnorm(15,0,0.25)
hybrid=pots+effect+noiseh
autoz=pots+noisea
```

Perform both a simple t.test and a paired t.test, which is more powerful
in this case?

```{r}
t.test(hybrid,autoz,paired=FALSE)
t.test(hybrid,autoz,paired=TRUE)
```

Maybe we were just lucky in our random simulation here.

Check which method is more powerful. Run a parametric bootstrap
experiment, generate data as above B=1000 times and compute the average
probability of rejection for these 1000 trials, with a false positive
rate $\alpha=0.05$.

```{r bootstrapPower}
B=1000 ; n=15 ;effect=0.2
ppaired=rep(0,B)
pttest=rep(0,B)
for (i in 1 :1000){
pots=rnorm(15,0,1)
noiseh=rnorm(15,0,0.25)
noisea=rnorm(15,0,0.25)
hybrid=pots+effect+noiseh
autoz=pots+noisea
pttest[i]=t.test(hybrid,autoz,paired=FALSE)$p.value
ppaired[i]=t.test(hybrid,autoz,paired=TRUE)$p.value}
sum(pttest<0.05)/B
sum(ppaired<0.05)/B
```

We can plot the p-values obtained using both methods:

```{r pvaluescompare,fig.height=5,fig.width=4}
library("ggplot2")
dtp=data.frame(pvalues=c(pttest,ppaired),
    experiment=factor(c(rep("notpaired",B),rep("paired",B))))
m = ggplot(dtp, aes(pvalues, fill=experiment))
m + geom_histogram(binwidth=0.01,alpha=0.3)
```

# Exercises   

- Make a function which compares the power of the two types of tests
for different values of the effect size, sample size, sd in the pots.
- Use your function to find out which of the standard deviations (pots
or noise) has the largest effect on the improvement produced by pairing
for n=15.
- How big should n be to attain a power of 80% if the two standard
deviations

```{r powerPairedUnpaired,fig.height=5,fig.width=4}
powercomparison = function(effect=0.2,n=15,B=1000,
                noisesd=0.25,potsd=1){
ppaired=rep(0,B);pttest=rep(0,B)
for (i in 1 :B){
pots=rnorm(n,0,potsd)
noiseh=rnorm(n,0,noisesd)
noisea=rnorm(n,0,noisesd)
hybrid=pots+effect+noiseh
autoz=pots+noisea
pttest[i]=t.test(hybrid,autoz,paired=FALSE)$p.value
ppaired[i]=t.test(hybrid,autoz,paired=TRUE)$p.value
}
PowerPaired=sum(ppaired<0.05)/B
PowerUnpaired=sum(pttest<0.05)/B
powers=cbind(PowerPaired,PowerUnpaired)
return(powers)
}
```

Here are a few values showing that when the pots sd is smaller than the
noise sd, pairing hardly makes a difference. If the pots variability is
larger than that of the measurement noise then pairing makes a big
difference.

```{r}
powercomparison(potsd=0.5,noisesd=0.25)
powercomparison(potsd=0.25,noisesd=0.5)
powercomparison(potsd=0.25,noisesd=0.1)
powercomparison(potsd=0.1,noisesd=0.25)
```

For 100 plants of each type and the two SDs being 0.5, the power of the
paired test is about 80%.

```{r}
powercomparison(potsd=0.5,noisesd=0.5,n=100)
```

take into account a natural pairing of the observations -- for instance,
twin studies, or studies of patients before and after a treatment. What
can be done when pairing is not available.

try to create pairs of subjects that have as much similarity as possible
through mathcing age, gender, background health etc. One is treated, the
other serves as a control.


# "Block what you can, randomize what you cannot" 
##  (George Box, 1978)

Special designs: paired, complete block randomized, 

Often we don't know which nuisance factors will be important, or we
cannot plan for them ahead of time. 

In such cases, randomization is a
practical strategy: at least in the limit of large enough sample size,
the effect of any nuisance factor should average out.

## Complete Random Block Design (CRB)
![Complete randomized](../../images/CompleteRB3.png){width="50%"}

## Balanced incomplete Block Design ()
![balanced incomplete](../../images/balincomblock.png){width="50%"}

## Complete factorial Latin Squares
![sudoku anyone?](../../images/Latinsquare9.png){width="50%"}


# Randomization decreases bias.

 - Humans are bad at assigning treatments truly at random.  
 - Random assignment reduces unconscious bias 
    (special samples treated differently, balancing things out,..)
 - Randomization also helps with unknown nuisance factors.    

## Randomization helps inference.

 - if the sample is randomly generated from a population, we can infer
   something about the population we drew from.
  
## Random does not mean haphazardly:
 - Need to use a random number generator and a seed.
   
# Controls, positive and negative: why?

We often need to remove variation due to unknown factors, or decompose
variability according to its different sources; this is classically done
using analysis of variance and mixed models that can accomodate random
factors such as subject effects and fixed factors such as batch effects.

Usually these decompositions require at least 3 replicate measurements
in each `cell`. 

Removal of effects from unknown sources can only be done
through the use of **negative controls**[^9]. 



Calibration of the effect size in an experiment also requires the use of 
**positive controls**; spike-ins 
(for instance External RNA Control Consortium
controls as used in @Risso2014) where a known quantity or a known
expression level aid in these calibrations and are a standard part of
many experimental protocols.

Validation in independent data / independent lines of argument

If it is too good to be true, it usually is? An anecdote [^10]

# How many replicates do I need?

We use preliminary data and  simulation
experiments calculating how many nucleotides were necessary to achieve a
80% true positive rate when we knew the alternative. 

Now, recall the
discussion of experiments versus studies. 

For the cell line experiment, we might
get the correct result already from one replicate; usually we'll do two
or three to be sure. 

On the other hand, for the study, our intuition
tells us that there is so much uncontrolled variability that 20 is
likely far too few, and we may need 200 or 2,000 patients for a reliable
result. The number of replicates needed is highly context specific. It
depends on the amount of uncontrolled [^11] variability, and the
<span>effect size</span>. A pragmatic approach is to check out previous
successful (or unsuccessful) experiments or studies that did something
comparable and use simulations, subsampling or bootstrapping to get an
estimate of the prosed study's power. Here are more details about how to
go about this in practice.

# Power depends on sample sizes, effect sizes and variability.

 - Example of two batches with different variability.
 - Examples of one treatment where the variance is larger.
 - Need to accomodate **heteroscedasticity==unequal variances** with 
 different sample sizes.

# Effective sample size for dependent data.

 -  Dependent data require **larger** samples
    (less information: the standard errors of mean/median are larger)
    
 - The mean of 64 iid measurements has a smaller standard error than that of
    64 repeated measurements on a same patient (because of their correlation).
    
Example:
To have a power of 80% to distinguish two groups ($\alpha=0.05$)
one needs samples of size 64 in each group (128 measurements).\\

For two time points that have a correlation of $\rho=0.6$ one needs
a sample of $N=50$ measured twice (ie 200 measurements ).

 -  Equalize the variance by taking  more samples when higher variability.


![Perturbation Design](../../images/SamplingDesignSmall.png){width="60%"}
# Generalization of the idea of more points close to perturbation

## Response surface design

## Sequential sampling design
### Predefined stopping rules
### Change-point detection
## 

## Identifiability

Suppose we have two possible scenarii and 
we are collecting data to distinguish between them:

Say our observations are the sum of two
normals with unknown means and variances 1:

we see

```{r echo=FALSE}
x1=rnorm(50,1,1)
y1=rnorm(50,3,1)
x1+y1
```

Same observables, different source representations.

Another example ;
a mixture of two binomials:
$$\pi B(2,p_1)+ (1-\pi) B(2, p_2)$$
```{r}
prob=0.7
weights=rbinom(100,size=1,0.7)
bs=rbinom(100,size=2,prob=c(0.3,0.5)[weights+1])
bs
```

We see an overall binomial but have 3 parameters
and only two observations.

Often we can tell that a system is unidentifiable
if there are too many parameters given the amount
of data; that's why adding constraints can solve the problem.


# Never discard data: admissibility

Sometimes software packages offer to automatically
discard data to **equalize** variances.

This is always a bad idea.

We can subsample, as we do in the bootstrap, but then do it many times
to maintain the information about the variability.

# Simulating for power calculations....

Example of using some existing R functions from the `pwr` package:

```{r}
library("pwr")
pwr.t.test(n=15,d=0.4,sig.level=0.05,type="two.sample")
pwr.t.test(n=15,d=0.4,sig.level=0.05,type="paired")
pwr.t.test(d=0.4,sig.level=0.05,type="two.sample",power=0.8)
pwr.t.test(d=0.4,sig.level=0.05,type="paired",power=0.8)
```
Some power calculations need more than the sample
and effect size, they alse need the relative size of the 
standard deviation to the mean, called the coefficient of
variation $cv=\sigma/\mu$.

For highthroughput studies such RNA-seq the package
`RNASeqPower` has the same type of input.
However, there are more important quantities
to take into account:
- The depth of sequencing and consequent expected count for a given transcript, argument depth. 
-  The coefficient of variation of counts within each of the two groups, argument cv.     
- The relative expression that we wish to detect $\Delta$, argument effect.
-  The target false positive rate $\alpha$ and false negative rate 
$\beta$ or power $= 1-\beta$, arguments alpha and power.
- The number of samples n in each group, argument n

We often do not know some of the true values for our input
so we do our calculcations at many tentative values, creating
an abaque or grid of possible values within the limits
that we think will cover our experiment:

```{r}
library("RNASeqPower")
rnapower(depth=20, cv=.4, effect=c(1.25, 1.5, 1.75, 2),
             alpha= .05, power=c(.7,.8, .9))
```
```{r}
rnapower(depth=200, cv=.4, effect=c(1.25, 1.5, 1.75, 2),
             alpha= .05, power=c(.7,.8, .9))
```
We see a table of values. 

## Example: prove that it's better to run
##a longitudinal study than a separate control group

Many power simulations cannot be done using these
standard functions because they have the extra levels of complexity
of your specific experimental set-up.

A recent response-to-reviewers calculation we had to do:

![Simulation design choice](/Users/susan/Books/CUBook/images/S5-simulation_control_data_subset.png)


# Incomplete blocks, ragged arrays, missing data.

```{css}
.red { background-color: red; }
Data Imputation...
```

# Mean-Variance Relationships and Transformations

Previously we saw examples for data
transformations whose that compress or stretch the space of quantitative
measurements in such a way that the measurements' variance is more
similar throughout. Thus the variance is no loner highly dependent on
the mean value[^14].

The mean-variance relationship of a measurement technology can in
principle be any function, but in many cases, the following prototyic
relationships hold:

1.  constant: the variance is independent of the mean.

2.  Poisson: the variance is proportional to to the mean.

3.  quadratic: the standard deviation is proportional to the mean,
    therefore the variance grows quadratically.

Give examples for biological assays (or measurement technologies) whose
data show these types of relationships.

Real data can also be affected by a combination of these. For instance,
with DNA microarrays, the fluorescence intensities are subject to a
combination of background noise that is largely independent of the
signal, and multiplicative noise whose standard deviation is
proportional to the signal [@RockeDurbin:2001]. Therefore, for bright
spots the multiplicative noise dominates, whereas for faint ones, the
background.

Load up the raw data from a microarray experiment with replicates and
verify the above statement.

We recall from Lecture 4 that for data with a mean-variance
relationship $v(\mu)$ the variance-stabilizing transformation
$g:\mathbb{R}\to\mathbb{R}$ fulfills the condition
$$g'(x)=\frac{1}{\sqrt{v(x)}}$$

a\) What are the variance-stabilizing transformations associated with the
above three prototypic mean-variance relationships?\
b) What is the variance stabilizing transformtaion which is appropriate
for $v(\mu) = v_0 + c\,\mu^2$, where $v_0>0$ is a positive constant?

Data quality assessment and quality control {#design:sec:quality}
-------------------------------------------

Data quality assessment (QA) and quality control (QC, i.e., the removal
of insufficiently good data) are essential steps of any data analysis.
Being critical of data quality (both of raw and derived data) should
pervade all phases of analysis, from data import over model fitting,
hypothesis testing to interpretation. The most useful tool for QA is
usually visualisations. If they highlight anomalies, it's
necessary to decide which remedial steps to take (for instance, to
exclude certain parts of the data), and, probably, to redo the analysis,
either with the same or a refined method. These decisions are
necessarily subjective and context-dependent.

It is helpful to be clear on what one means by <span>quality</span>, as
the word can have many meanings. The most pertinent for us is
<span>fitness for purpose</span>[^15]. Back to the example of RNA-Seq
data, the purpose of the experiment is, in many cases, the detection of
differentially expressed genes between different biological conditions.
The aim of QA/QC will then be the identification of data points or
groups of data points (e.g., all data from one sample, all data from one
gene) that apparently suffered from an anomaly that makes them
detrimental to this purpose. Ordination plots and heatmaps  are useful to identify outliers: for
instance, samples that are not behaving as expected, because of a sample
swap or misannotation; or genes that were not measured properly.


Longitudinal Data
-----------------

Longitudinal data[^16] have time as a covariate. The first question is
whether we are looking at a handful of time points -- say, the response of
a cell line measured 48h, 72h and 84h after exposure to a drug; or a
long and densely sampled time series such as patch clamp data in
electrophysiology or a movie from life cell microscopy.

In the first case, time is usually best thought of as just another
experimental factor, in the same way as we consider the concentration or
the choice of drug. One analysis strategy could be to first identify the
"best", or biologically most indicative, time point, and then focus on
that. Or we can ask whether there is any effect at all, regardless of
the time. We then just need to make sure that we account for the
dependencies between the measurements over time, and effective sample
sizes. When designing the
experiment, we'll also try to sample more densely at those times when we
expect most to happen.

In the second case, time series, we'll often want to fit dynamical
models to the data. We have many choices:

-   (Hidden) Markov models

-   Change point detection

-   Ordinary differential equations or reaction-diffusion models

-   Piece-wise deterministic stochastic processes

-   Autoregressive models

-   Non-parametric smoothing followed by clustering or classification
    into prototypic shapes

# Don't Pretend You Are Dumb

There is some attraction to seemingly "unbiased" approaches that analyse
the data at hand without any reference to what is already known. Such
tendencies are reinforced by the fact that statistical methods have
often been developed to be generic, for instance, working of a general
matrix without specific reference to what the rows and column might
signify.

Generic approaches are a good way to get started, and for analyses that
are highly powered and straightforward, such an an approach might work
out. But often, it is wasteful. Recall the example of an RNA-Seq
experiment for differential expression. As we saw in Chapters
@(Chap14) and @(Chap7), we could perform a hypothesis test for
differential expression for each annotated gene, regardless of its read
counts, and then run a multiple testing method that treats all tests as
exchangeable. But this is inefficient - we can improve our detection
power by filtering out, or downweighting, the tests with lower
signal-to-noise ratio.

Other examples include:

-   Penalisation or feature selection in high-dimensional regression or
    classification. It is easy to use schemes that treat all features
    the same, for instance, standardize all of them to zero mean and
    unit variance. But sometime we know that some classes of features
    are likely to be more or less informative than
    others.

-   You can also use graphs to represent "other" data (networks) and use
    approaches like the group or graph lasso to
    structure your penalties in high-dimensional modelling.

-   Unsupervised clustering of your objects of interests (samples,
    genes, etc.) and subsequent search for over-represented annotations.
    Instead, just check whether the members of groups defined by the
    annotations are more similar than two randomly picked objects.

# Sharpen Your Tools: Reproducible Research


Analysis projects often begin with a simple script, perhaps to try out a
few initial ideas or explore the quality of the pilot data. Then more
ideas are added, more data come in, other datasets are integrated, more
people become involved. Eventually the paper needs to be written,
figures be redone properly, and the analysis be saved for the
scientific record and to document its integrity. Here are a few tools
that can help with such a process.

**Use an integrated development environment.** <span>RStudio</span> is a
great choice; there are also other platforms such as Emacs or Eclipse.

**Use literate programming** tools such as <span>Rmarkdown</span> or
Jupyter. This is more readable (for yourself and for others) than
burying explanations and usage instructions in comments in the source
code or in separate README files, in addition you can directly embed
figures and tables in these documents. Such documents are often good
starting points for the supplementary material of your paper.

**Anticipate re-engineering of the data formats and the software.** The
first version of how you choose to represent the data and structure the
analysis workflows will rarely be the best. Don't be afraid[^18] to make
a clean cut and redesign them as soon as you notice that you are doing a
lot of awkward data manipulations or repetitive steps. This is time
well-invested. Sometimes it also helps to unearth bugs.

**Reuse existing tools.** Don't reinvent the wheel and rather spend your
time on things that are actually new. Before implementing a 'heuristic'
or a temporary hack that analyses your data in a certain way, spend a
couple of minutes researching to see if something like this hasn't been
done before. More often than not, it has, and there is a clean,
scalable, and already tested solution.

**Use version control**, such as git. This takes some time to learn, but in
the long run will be infinitely better than all your self-grown attempts
at managing evolving code with version numbers, switches and the like.
Moreover, this is also the sanest option for collaborative work on code,
and it provides an extra backup of your codebase, especially if the
server is distinct from your workplace machine.

**Use functions** rather than copy-pasting 
stretches of code.

**Use the R package system.** Soon you'll note recurring function or
variable definitions that you want to share between your individual
scripts. It is fine to use the R function to manage them initially, but
it is never to early to move them into your own package at the latest
when you find yourself starting to write README files or long emails
explaining others how to use some script or another. Assembling existing
code into an R package is not hard by any means, and offers you many
goodies including standardized and convenient ways to provide
documentation, to show code usage examples, to test the correct
functioning of your code, and to version it. Quite likely you'll soon
appreciate the benefit of using namespaces.

**Centralize the location of the raw data files and streamline the
derivation of intermediate data.** Store the input data at a centralized
file server that is professionally backed up. Mark the files as
read-only. Have a clear and linear workflow for computing the derived
data (e.g. normalised, summarised, transformed etc.) from the raw files,
and store these in a separate directory. Anticipate that this workflow
will need to be re-run several times, and version it. Use or similar
tools[^19] to mirror these files on your personal computer.

**Integration.** When developing downstream analysis ideas that bring
together several different data types, you don't want to do the
conversion from data type specific formats to the representations that
machine learning or generic statistical methods use each time on an ad
hoc basis. Have a *recipe* script that assembles the different
ingredients and cooks them up as an easily consumable[^20] matrix, data
frame or Bioconductor .\
**Keep a hyperlinked webpage with an index of all analyses.** This is
helpful for collaborators (especially if the page and the analysis can
be accessed via a web browser) and also a good starting point for the
methods part of your paper. Structure it in chronological or logical
order, or a combination of both.

# Data representation


Combining all the data so it is ready for analysis or visualisation often involves a lot of
shuffling around of the data, until they are in the right shape and
format for an analytical algorithm or a graphics routine. 

Errors can occur, lost labels, lost information: be safe, redundancy is good.



# Wide vs long table format

Recall Hiiragi data (for space reasons we print only the first five
columns):



```
    ##             1 E3.25  2 E3.25  3 E3.25  4 E3.25  5 E3.25
    ## 1420085_at 3.027715 9.293016 2.940142 9.715243 8.924228
    ## 1418863_at 4.843137 5.530016 4.418059 5.982314 4.923580
    ## 1425463_at 5.500618 6.160900 4.584961 4.753439 4.629728
    ## 1416967_at 1.731217 9.697038 4.161240 9.540123 8.705340
```

This dataframe has several columns of data, one for each sample
(annotated by the column names). Its rows correspond to the four probes,
annotated by the row names. This is an example for a data table in *wide
format*.

```
    ##   variable    value
    ## 1  1 E3.25 3.027715
    ## 2  1 E3.25 4.843137
    ## 3  1 E3.25 5.500618
    ## 4  1 E3.25 1.731217
    ## 5  2 E3.25 9.293016
    ## 6  2 E3.25 5.530016
```
In the resulting dataframe , each row corresponds to exactly one
measured value, stored in the column . Then there are additional columns
variable and value, which store the associated covariates.

Compare this to the dataframe above.

Now suppose we want to store somewhere not only the probe identifiers
but also the associated gene symbols. We could stick them as an
additional column into the wide format dataframe, and perhaps also throw
in the genes' ENSEMBL identifier for good measure. But now we
immediately see the problem: the dataframe now has some columns that
represent different samples, and others that refer to information for
all samples (the gene symbol and identifier) and we somehow have to
know this when interpreting the dataframe. This is what Hadley Wickham
calls *untidy data*[^21]. In contrast, in the tidy dataframe , we can
add these columns, yet still know that each row forms exactly one
observation, and all information associated with that observation is in
the same row.

#### Ragged arrays

In tidy data @Wickham,

1.  each variable forms a column,

2.  each observation forms a row,

3.  each type of observational unit forms a table.

A potential drawback is efficiency: even though there are only 4 probe-gene symbol relationships, we are now storing them 404 times in the rows
of the dataframe . Moreover, there is no standardisation: we chose to
call this column , but the next person might call it or even something
completely different, and when we find a dataframe that was made by
someone else and that contains a column , we can hope, but have no
guarantee, that these are valid gene symbols. Addressing such issues is
behind the object-oriented design of the specialized data structures in
Bioconductor, such as the class.

# Matrices versus dataframes

For a specific data type, it may not always be the most efficient way of
storing data, and it cannot easily transport rich metadata (i.e., data
about the data).[^22] For instance, our example dataset is stored as an
object in Bioconductor's class SummarizedExperiment which has multiple components, most
importantly, the matrix with 45101 rows and 101 columns. The matrix
elements are the gene expression measurements, and the feature and
sample associated with each measurement are implied by its position
(row, column) in the matrix; in contrast, in the long table format, the
feature and sample identifiers need to be stored explicitly with each
measurement. Besides, has additional components, including the
dataframes and , which provide various sets metadata about the
microarray **features** and the phenotypic information about the
samples.

# Analysis Workflow Design

Don't immediately rush into downstream ('biological') analysis, or run
black-box algorithms. First make sure you understand the qualities of
the data. Here are some questions and diagnostic plots:

-   How do the marginal distributions of the variables look (histograms,
    ECDF plots)?

-   How do their joint distributions look (scatter plots, pairs plot)?

-   How well do replicates agree (as compared to different biological
    conditions)? Are the magnitudes of the differences between several
    conditions plausible (e.g., do two similar conditions also show more
    similar data than two very different conditions?)

-   Is there any evidence for batch effects? These could be of a
    categorical (stepwise) or continuous (gradual) nature, e.g. due to
    changes in experimental reagents. Factors associated with these
    affects could be explicitly known (e.g., different, chronologically
    and organisationally separated screening campaigns), unkown or
    latent (e.g., population substructure in a genetic study that was
    not explicitly recorded), or somewhere in between (e.g., potential
    degradation over time of a measurement device, when we have recorded
    the measurement times but don't really understand the degradation
    process quantitatively).

-   For many of the last two sets of questions, heatmaps, principal
    component plots and other ordination plots  are helpful.

Many examples of expression experiments with unfortunate designs exist.
@LinLinSnyder had an experimental design where the human and mouse
samples were treated differently (different lanes, time to collect
samples etc..) so that their <span>confounding</span>[^23] of batch
factor and species factor precluded the verification of a difference
between species [@Gilad2015].


# Leaky pipelines and statistical sufficiency![](../../images/leakypipeline.png){width="25%"}

Data analysis pipelines in high-throughput biology often work as
*funnels* that successively summarise and compress the data. In
high-throughput sequencing, we may start with individual sequencing
reads, then align them to a reference, then only count the aligned reads
for each position, summarise positions to genes (or other kinds of
regions), then ???normalize??? these numbers by library size to make them
comparable across libraries, etc. At each step, we loose some
information, and it is important to make sure we still have enough
information for the task at hand[^24]. The problem is particularly
burning if we use a data pipeline built from a series of separate
components without enough care being taken ensuring that all the
information necessary for ulterior steps is conserved.

Statisticians have a concept for whether certain summaries enable the
reconstruction of all the relevant information in the data:
<span>sufficiency</span>. In a Bernoulli random experiment with a known
number of trials, $n$, the number of successes is a sufficient statistic
for estimating the probability of success $p$.

In a 4 state Markov chain (A,C,G,T) such as the one we saw in
Chapter??@(Chap10), what are the sufficient statistics for the
estimation of the transition probabilities?

Iterative approaches akin to what we saw when we used the EM algorithm
can sometimes help avoid information loss. For instance, when analyzing
mass spectroscopy data, a first run guesses at peaks individually for
every sample. After this preliminary spectra-spotting, another iteration
allows us to borrow strength from the other samples to spot spectra that
may have been labeled as noise.



<span>Parametric bootstrap for power simulations</span> Power
calculations need to be done before an experiment is run. However, when
calculating power one needs to know estimates of quantities such as the
effect size[^26] (the difference in means between two treatments) that
will only be known after the experiment. The best way to work around
this is to simulate with different possible values of the unknown
parameters to get an idea of the actual sample sizes needed.

Generate data with:

-   same error distribution (shape and moments), taking into all the
    different sources of error that might affect our data

-   various effect sizes

-   various choice of experimental design: sample sizes or number of
    replicates, sequencing depth

To do this, we need to find a parametric family $F_\theta$ that is not
too terrible in modeling our data. Then we estimate $\theta$ from the
data generate whole sets of simulated data from the distribution
$F_{\hat{\theta}}$, while varying effect sizes, sample sizes, levels of
replication, etc.




# Summary


Issues  should be carefully considered
before doing an experiment. 

There are two types of variation in an
experiment: one is of interest, the other is unwanted. 

We usually cannot
rid ourselves of all the unwanted variation but we saw how we can used
balanced randomized designs, data transformations that improve quality
control. 

We showed how to compute the power of our study by simulation
experiments, these illustrated the necessity to carefully choose time
points in longitudinal studies especially when perturbations are part of
the design.

A reproducible workflow enables us to be transparent in our choices and
enable us to evaluate the overall robustness of the results.

# Technology specific issues

## RNA-seq type experiments, metagenomics, chipseq

-   Trade-offs: library size (depth) versus number of samples

-   Pooling or bar coding? Negative controls. @auer2010statistical

Read Coverage Depth Choice depends on the context and goal of the
experiment. @sims2014sequencing make some very important points about
choices that have to be made before the experiment is carried out. With
a finite number of runs and samples possible because of resource
limitations, one had to ...

# Different problems require different depths

-   De novo genome sequencing,

-   Genome resequencing,

    -   SNV (deepSNV, STR, indels) : heterozygous SNVs detected through
        33x depths

    -   Clonal evolution in cancer samples can require 1000x depths
        (variants present in less than 1%)

    -   Disease causing recessive variants requires parent-child
        sequences

-   Transcriptome sequencing

-   Genomic location analyses (ChIP???seq)

-   Metagenomics

    -   All genomes present

    -   Phylogenetic study (16S rRNA)

-   Problems with uninformative reads

    -   RNAseq (low quality or rRNA genes)

    -   Unknown taxa in species studies

    -   Unmapped reads (ChIP-seq, RNAseq)
 
    -   Problems with multiplexing
    
# Mass spec, proteomics

See careful study by  @oberg2009statistical

# References and Further Reading

    This lecture presented merely a pragmatic introduction to design,
    there are many book long treatments of the subject that offer
    detailed advice on setting up
    experiments 
    
@wu2011experiments; @box1978statistics; @glass2007experimental

