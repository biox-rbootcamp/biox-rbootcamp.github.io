---
title: "Clustering"
author: "Susan Holmes (c)"
date: "June, 2017"
output:  
   slidy_presentation:
     fig_width: 6
     fig_height: 5
     fig_retina: 1
     keep_md: true
---



```{r,echo=FALSE}
require(knitr)
opts_chunk$set(cache=TRUE, cache.path = 'ClusterLectureBasic_Cache/', fig.path='figure/clust-')
```

![Birds of a Feather](../../../images/flock-of-starlings.jpg)

Finding  a latent or hidden variable present which we are not necessarily provided.

Clusters do exist in Biomedical Data
======================================

![Cancer Clusters](../images/CancerCluster.jpg)

A recent example of effective use of clustering is the realization that
even a disease such as Breast Cancer does not have a single molecular signature but
there are many Molecular  type of Cancers.

Also immune cells seen through flowcytometry
=============================================

![FlowCytometry CD4/CD3](../images/FlowCytoTcells.gif)



Data Augmentation
=====================

This is a case where we can benefit from putting the data in an augmented format:
$(X,g)$ where $g$ is the unknown cluster the data come from.
The variable $g$ is categorical, taking on as many different
values as there are clusters.

Methods that add useful but unknown components to the data like this
are called `data augmentation` methods



There are already thousands of existing clustering algorithms combining
different choices for the basic distance used to compare observations,
ways of combining similar observations and ways of deciding how many cluster exist.

We decompose these choices according to the following steps starting from an observation by feature matrix X, we have to choose 
a distance matrix from which to construct the clusters, hoping that they are well separated.

![Clustering Workflow](../images/ClusterWorkflow.png)



Clustering for Pattern Recognition 
=======================================
![LondonBombings](../images/LondonBombings.jpg)


![Cholera Map](../images/SnowMap_Points.jpg)


![Breast Cancer](../images/Breastcancerclusters.JPG)



Clustering : Grouping by similarity
==========================================
**Of a feather**
How are distances measured, or how is
similarity between observations defined ? 


Distances
========================
- Euclidean Distances SQRT(Sums of squares of coordinates).
- Mahalanobis distance (unequal weight per direction).
- Weighted euclidean distances, $\chi^2$,...
- Manhattan/Hamming/City Block.
- Measurements of co-occurrence, ecological/
sociological data for instance **Jaccard**. When what really counts is
how often certain species are found together then
if the observations are just sequences of 0's and 1's,
presence of 1's in the same spots does not present the
same importance as that of the 0's: Jaccard distance=1-JC.

- Distances between `purified ' observations
(we transform the data first).

There are almost a hundred different distances available combining
outside information (distance on a graph, geodesic distance along a path,
distance on a tree), combining categorical data and continuous data
(Gower's distance) and using many different weighting schemes.

If you know what is the relevant notion of `closeness' or similarity
for your data, you have (almost) solved the problem.

Distance functions available
===============================

[phyloseq distances](http://joey711.github.io/phyloseq/distance.html)

```{r duneDist} 
require(vegan)
data(dune)
dist.dune=vegdist(dune)
symnum(as.matrix(dist.dune))
```

```{r heatmapDist,echo=FALSE,fig.width=9,fig.height=9,warning=FALSE} 
require(graphics)
require(gplots)
#rc <- rainbow(40, start=0, end=0.99)
rc= heat.colors(21, alpha = 1)
dr=round(as.matrix(dist.dune),1)
#heatmap.2(as.matrix(dist.dune),symm = TRUE, margins = c(6,6),Rowv = NA, Colv = NA,col=bluered,trace="none")
heatmap.2(1-as.matrix(dist.dune),symm = TRUE, margins = c(3,3),Rowv = NA, Colv = NA,col=rc,distfun=function(c) as.dist(c),
          trace="none",cellnote=dr)
```


```{r ggplotdistheatmap,eval=FALSE,echo=FALSE}
library(ggplot2)
library(RColorBrewer)
jBuPuFun <- colorRampPalette(brewer.pal(n = 9, "BuPu"))
paletteSize <- 256
jBuPuPalette <- jBuPuFun(paletteSize)
dd=as.matrix(dist.dune)
prdune <- data.frame(sample = colnames(dd),
                        probe = rownames(dd),
                        dist = dd)
ggplot(prdune, aes(x = probe, y = sample, fill = dist)) +
  geom_tile() +
  scale_fill_gradient2(low = jBuPuPalette[1],
                       mid = jBuPuPalette[paletteSize/2],
                       high = jBuPuPalette[paletteSize],
                       midpoint = (max(prdune$dist) + min(prdune$dist)) / 2,
                       name = "Distance")
```

%\includegraphics[width=3.2in,height=4in]{xkcdbirds_and_dinosaurs.png}





Hierarchical Clustering
==========================
When the number of clusters is not known a priori,
it is useful to perform hierarchical clustering because
it provides a graphical summary that enables us to evaluate where the
cutoff for choosing clusters should occur.

This class of methods starts with all the observations making their own cluster and agglomerates gradually the
points together by similarity so the number of clusters decreases as points are clumped together 
into clusters, of course the user will have to make choices:

1. What is the relevant distance between observations to start with.
2. Which criteria should be used for grouping  clusters that have been composed. 
3. How should distances between these clusters be defined.


![Like a mobile](../images/CalderMobile.jpg)


# Methods of computing dissimilarities between the aggregated clusters

- Minimal jump (Single linkage or nearest neighbor):
 This will separate the groups as much as possible,
$$ D_{12}=min_{i \in C_1, i \in C_2 } d_{ij}. $$

String objects together to form
clusters, and the resulting clusters tend to represent long
``chains.'' or ``combs''. 

- Maximum jump (or Complete linkage): 
(gives compact groups)\\
Furthest neighbor: the distances
between clusters are determined by the greatest distance between any
two objects in the different clusters. 
This method usually performs quite well in cases when
the objects actually form naturally distinct `clumps`. If the
clusters tend to be somehow elongated or of a `chain` type nature,
then this method is inappropriate. 

- Mean : half way between the two above, 
 $$ D_{12}=\frac{1}{|C_1| |C_2|}\sum_{i \in C_1, i \in C_2 } d_{ij}$$
Unweighted pair-group average. 
(Goldilocks method)

- Weighted pair-group average. 

- Ward's method. An analysis of variance approach to evaluate the distances
between clusters. 
 Minimize the Sum
of Squares (SS) of any two (hypothetical) clusters that can be formed
at each step. 
This method is regarded as very efficient,
however, it tends to create clusters of small size. 

```{r}
require(vegan)
hclust.aver <- hclust(dist.dune, method="aver")
plot(hclust.aver)
```

Can't resist this here:
==========================

```{r vegemite}
vegemite(dune, hclust.aver)
```

```{r heatmapwithimage}
my.hclustfun=function(d){hclust(d, method = "aver")}
my.distfun=function(x) {vegdist(x)}
color = rev(heat.colors(256))
heatmap.2(as.matrix(dune),hclustfun=my.hclustfun,distfun=my.distfun,col=color)                      
```


# Advantages and Disadvantages of the various distances between clumps
Tree shapes differ:
![](../images/HC3.jpg)

# Single linkage
Good for recognizing the number of clusters .... $\ldots$ ... But combs.

```{r singlelinkhc}
my.hclustfun=function(d){hclust(d, method = "single")}
my.distfun=function(x) {vegdist(x)}
heatmap.2(as.matrix(dune),hclustfun=my.hclustfun,distfun=my.distfun,col=color) 
```

# Maximal linkage
Compact classes  .... one observation can alter groups.

```{r completelink}
my.hclustfun=function(d){hclust(d, method = "complete")}
my.distfun=function(x) {vegdist(x)}
heatmap.2(as.matrix(dune),hclustfun=my.hclustfun,distfun=my.distfun,col=color) 
```


# Average
Classes have similar size and variance.

```{r heatmap2withimage}
my.hclustfun=function(d){hclust(d, method = "aver")}
my.distfun=function(x) {vegdist(x)}
color = rev(heat.colors(256))
heatmap.2(as.matrix(dune),hclustfun=my.hclustfun,distfun=my.distfun,col=color)                      
```

#### Centroid
More robust to outliers.

#### Ward
- Minimising an inertia (sums of squares of distances within groups).
- Classes are small if high variability observations.



Computing and interpreting hierarchical clustering trees
========================================================

```{r hclustdune}
require(vegan)
hclust.aver <- hclust(dist.dune, method="aver")
plot(hclust.aver)
```



Non parametric Mixture Detection: k means and k-medoids
=========================================================
These methods are known as non-hierarchical Clustering or iterative relocation methods.
There are several initial choices to be made
with these methods and when the a priori knowledge
is not available this can be a drawback. The first is
the number of clusters suspected.
Each time the algorithm is run, initial `seeds'
for each cluster
have to be provided, for different starting configurations
the answers can be different.

The function  *kmeans* is the one that can be used in *R*.

Example of how `kmeans` works
=============================
```{r kmeansa,fig.show="animate",eval=FALSE}
require(animation)
kmeans.ani(x = matrix(c(runif(100),runif(100)), ncol = 2, dimnames = list(NULL, c("X1", "X2"))), centers = 3, pch = 1:3,col = 1:3, hints = c("Move centers", "Find clusters"))
```

[Animated Gif](http://shabal.in/visuals/kmeans/2.html)

```{r,warning=FALSE}
#set.seed(95647745)
set.seed(952224887)

Xmat=matrix(runif(200),ncol=2)
nk=3
cents=matrix(runif(2*nk),ncol=2)
###Default distance=Euclidean
dist1=function(vec){dist(rbind(vec,cents[1,]))}
dist2=function(vec){dist(rbind(vec,cents[2,]))}
dist3=function(vec){dist(rbind(vec,cents[3,]))}

dists123=cbind(apply(Xmat,1,dist1),apply(Xmat,1,dist2),apply(Xmat,1,dist3))
clust0=apply(dists123,1,which.min)



out1=kmeans(Xmat,cents,iter.max=1)
out2=kmeans(Xmat,cents,iter.max=2)
out3=kmeans(Xmat,cents,iter.max=3)
plot(Xmat,type='n')
text(Xmat[,1],Xmat[,2],out1$cluster)
text(Xmat,col=c('red','blue','green')[out1$cluster],out1$cluster)

```
```{r}
require(ggplot2)
data0=data.frame(x=Xmat[,1],y=Xmat[,2],clust=as.factor(clust0))
data1=data.frame(x=Xmat[,1],y=Xmat[,2],clust=as.factor(out1$cluster))
###Extra points, centers of clusters
cdg=data.frame(x=cents[,1],y=cents[,2],clust="c")
pp=ggplot(data=data0,aes(x= x, y = y, colour = clust, shape = clust))+
scale_shape_discrete(solid=T, guide=F) +
geom_point(data=data0, mapping=aes(x=x, y=y, shape=clust,colour=clust), size=5)+
 geom_point(data = cdg, colour = "black",size=5,shape=1)
pp

cents=out1$centers
cdg=data.frame(x=cents[,1],y=cents[,2],clust="c")
pp=ggplot(data=data1,aes(x= x, y = y, colour = clust, shape = clust))+
scale_shape_discrete(solid=T, guide=F) +
geom_point(data=data1, mapping=aes(x=x, y=y, shape=clust,colour=clust), size=5)+
 geom_point(data = cdg, colour = "black",size=5,shape=1)
pp

```
Tight Clusters (Nuees Dynamiques)
==================================
A more evolved method called Dynamical Clusters
is also interesting because it repeats the process many
times and builds what is known as `strong forms'
which are groups of observations that end up in
the same classes for most possible initial configurations.

Have to choose how many classes they will be prior to the analysis.
Can depend on the initial seeds, so we may need to repeat the analyses over and over again.


k-medoids algorithm
=====================

1. For a given cluster assignment C find the observation in the cluster 
minimizing total distance to other points in that cluster: 
$$i^*_k = \mbox{argmin}_{\{i:C(i)=k\} }\sum_{C(i* )=k }D(x_i , x_i ). $$
Then $m_k = x_{i^*_k} , k = 1, 2,\ldots , K$ are the current estimates of the 
cluster centers. 

2. Given a current set of cluster centers 
$\{m_1 , \ldots, m_K \}$, minimize the total error by assigning each observation to the closest (current) cluster
center: 
$$C (i) = \mbox{argmin}_{1 \leq k \leq K }
D(x_i , m_k ).$$

Iterate steps 1 and 2 until the assignments do not change. 









Some examples with simulated data
========================================================

Comparison of Truth, Kmeans Hierarchical Clustering
====================================================
```{r,echo=FALSE}
gr1 <- cbind(rnorm(100,0,2),rnorm(100,0,2))
gr2 <- cbind(rnorm(100,0,2),rnorm(100,8,2))
gr3 <- cbind(rnorm(100,8,2),rnorm(100,0,2))
gr4 <- cbind(rnorm(100,8,2),rnorm(100,8,2))
simul4 <- list(values=rbind(gr1,gr2,gr3,gr4),classes=c(rep(1,100),rep(2,100),rep(3,100),rep(4,100))) 
##op <- par(mfcol = c(2, 2))
#plot(simul4$values, col = as.integer(simul4$classes), xlim=c(-6,14), ylim = c(-6,14), xlab="", ylab="", main = "True Groups")
#####Compute kmeans classes with k=4
#km4 <- kmeans(simul4$values,4)
#plot(simul4$values, col = km4$cluster, xlab = "", ylab = "", main = "kmeans")
##############################################
```

```{r TrueGroups,echo=FALSE}
plot(simul4$values, col = as.integer(simul4$classes), xlim=c(-6,14),
     ylim = c(-6,14), xlab="", ylab="", main = "True Groups")
```

Compute kmeans classes with k=4
==================================
```{r KMgroups1}
#####
km4 <- kmeans(simul4$values,4)
plot(simul4$values, col = km4$cluster, xlab = "", ylab = "", main = "kmeans")
```

Plot the results of a hierarchical clustering assignment
=============================================================

```{r KMgroups2}
##############################################
hc = hclust(dist(simul4$values), method = "ward")
memb <- cutree(hc, k = 4)
plot(simul4$values, col = memb, xlab = "", ylab = "", main = "hclust Euclidean ward") 
```


Choosing the number of Clusters 
=======================================

```{r wss}
###############Compute the kmeans within group wss for k=1 to 9
wss = sum(apply(scale(simul4$value,scale=FALSE),2,function(x){x^2}))
for (k in 2:20) {
  km4 <- kmeans(simul4$values,k)
    wss = c(wss, sum(km4$withinss))
}
plot(1:20, wss, xlab='k', ylab='WSS(k)', type='l', lwd=2, col='red')
```



```{r logwss}
plot(1:20, log(wss), xlab='k', ylab='logWSS(k)', type='l', lwd=2, col='red')
```

```{r}
plot(hclust(dist(simul4$values), method = "ward"))
```

[Detailed graphical account](http://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters)


Silhouette Plots
=======================

For each observation i, the silhouette width s(i) is defined as follows: 
Put a(i) = average dissimilarity between i and all other points of the cluster to which i belongs (if i is the only observation in its cluster, s(i) := 0 without further calculations).

For all other clusters C, put d(i,C) = average dissimilarity of i to all observations of C. 

The smallest of these d(i,C) is called $$ b(i) := min_C d(i,C) $$ and can be seen as the dissimilarity between i and its “neighbor” cluster, i.e., the nearest one to which it does not belong. Finally,
$$
s(i) := ( b(i) - a(i) ) / max( a(i), b(i) ).
$$

```{r silhouette}
library(fpc)
library(cluster)
d=simul4$values
asw <- numeric(20)
for (k in 2:20)
  asw[[k]] <- pam(d, k)$silinfo$avg.width
k.best <- which.max(asw)
#cat("Silhouette-optimal number of clusters:", k.best, "\n")
k.best
p4=pam(x=d,k=k.best)
si=silhouette(p4)
plot(si,col="red",main="Simulated Data")
```

```{r dunesilhouette}
library(cluster)
asw <- numeric(20)
for (k in 2:18)
  asw[[k]] <- pam(dune, k)$silinfo$avg.width
k.best <- which.max(asw)
cat("Silhouette-optimal number of clusters:", k.best, "\n")
p4=pam(x=dune,k=k.best)
si=silhouette(p4)
plot(si,col="red",main="Dune Data")
```

```{r veganKM}
ccas <- cascadeKM(decostand(dune, "hell"), 2, 15)
```

Calinski-Harabasz variance ratio
=======================================

Within and between sums of squares:

![Between and Within](../images/BetweenWithinSmall.png)

W in black and B in Red.

Calinski and Harabasz, 1974:
$$(n-k)*sum(diag(B))/((k-1)*sum(diag(W)))\qquad B \mbox{between} W \mbox{within}$$


```{r, echo=FALSE}
pam.clustering = function(x, k) {
    # x is a distance matrix and k the number of clusters
    require(cluster)
    cluster = as.vector(pam(as.dist(x), k, diss = TRUE)$clustering)
    return(cluster)
}
data.dist=dist(simul4$values)
data=simul4$values
data.cluster = pam.clustering(data.dist, k = 3)
require(clusterSim)
nclusters = index.G1(data, data.cluster, d = data.dist, centrotypes = "medoids")
nclusters = NULL
for (k in 1:20) {
    if (k == 1) {
        nclusters[k] = NA
    } else {
        data.cluster_temp = pam.clustering(data.dist, k)
  nclusters[k] = index.G1(data, data.cluster_temp, d = data.dist, centrotypes = "medoids")
    }
}
```

```{r ClusterChoicePlot}
plot(nclusters, type = "h", xlab = "k clusters", ylab = "CH index", 
    main = "Optimal number of clusters: Simulated Data")
```    


```{r DuneDataCH}
data.dist=dist.dune
data=dune
data.cluster = pam.clustering(data.dist, k = 3)
require(clusterSim)
nclusters = index.G1(data, data.cluster, d = data.dist, centrotypes = "medoids")
nclusters = NULL
for (k in 1:19) {
    if (k == 1) {
        nclusters[k] = NA
    } else {
        data.cluster_temp = pam.clustering(data.dist, k)
        nclusters[k] = index.G1(data, data.cluster_temp, d = data.dist, centrotypes = "medoids")
    }
}
plot(nclusters, type = "h", xlab = "k clusters", ylab = "CH index", 
    main = "Optimal number of clusters: Dune Data")
```

Gene Expression Clustering
==========================
```{r}
#load("http://bios221.stanford.edu/data/Msig3transp.RData")
load("/Users/susan/Dropbox/CaseStudies/Msig3transp.RData")
#####You'll need to load the data from the data folder to your computer 
#####and then load it
dim(Msig3transp)
head(Msig3transp[,1:10])
dr=round(dist(Msig3transp),2)
heatmap(as.matrix(Msig3transp),labCol = "")
corM=cor(as.matrix(Msig3transp))
heatmap(corM,distfun=function(c)as.dist(1-c),labCol = "",labRow="")
corT=cor(as.matrix(t(Msig3transp)))
heatmap(corT,distfun=function(c)as.dist(1-c),labCol ="")
```


```{r}
load("/Users/susan/Dropbox/CaseStudies/Msigbinary.save")
head(disjonct)
heatmap(as.matrix(disjonct),distfun=function(c)dist(c,"manhattan"),labCol = "")
```

No need to invent a new Clustering Algorithm
==================================================

[too many already](http://cran.r-project.org/web/views/Cluster.html)


Diagnostic methods for numbers of clusters.
================================================================

- Gap Statistic
- Silhouette Plot 
- Calinski-Harabasz
- Looking for elbows in the WSS plots
- Look at the long edges in the hierarchical clustering.



Summary of Clustering Methods
===============================

- Choices of distances (garbage in, garbage out)
- Choices of assembly of classes criteria
- Choice of number of clusters
- A clustering algorithm will always try to cluster the data, even if the data aren't grouped.


A little politics ... as a transition
=================================================
```{r votingdata}
house=read.table("/Users/susan/Dropbox/CaseStudies/votes.txt")
head(house[,1:10])
party=scan("/Users/susan/Dropbox/CaseStudies/party.txt")
#table(party)
```
```{r distvote,echo=FALSE}
disthouse=dist(house,method="manhattan")/401
require(kernlab)
mycols=c("red","blue","green")
housem=as.matrix(house)
laplacedot=laplacedot(sigma=1/401)
house.K <- kernelMatrix(laplacedot, housem)
```

Plot of Votes 2005
=========================
```{r scatterplotvote}
require(scatterplot3d)
house.cmds=cmdscale(disthouse,k=20,eig=TRUE)
round(house.cmds$eig[1:20],1)
scatterplot3d(house.cmds$points[,1:3])
```

Plot of Votes 2005
=========================
```{r scatter3dcol}
scatterplot3d(house.cmds$points[,1:3],color=mycols[party+1])
```

Plot of Votes 2005
=========================
```{r kernelvoteplot}
house.kpca=kpca(house.K)
scatterplot3d(rotated(house.kpca)[,1:3])
```

Plot of Votes 2005
=========================
```{r kernelvoteplotcol}
house.kpca=kpca(house.K)
scatterplot3d(rotated(house.kpca)[,1:3],color=mycols[party+1])
```

Plot of Votes 2005 (interactive)
======================================

```
library(rgl)
plot3d(rotated(house.kpca),col=mycols[party+1])
```



[Multivariate methods](./PCA_SVD_Slides.html)